<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications by Topic</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Cong Ma</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="paper.html">Papers</a></div>
<div class="menu-item"><a href="paper_topic.html" class="current">Papers&nbsp;by&nbsp;topics</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="tutorial.html">Tutorials</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications by Topic</h1>
</div>
<h2>Low-rank matrix recovery</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2501.09336" target=&ldquo;blank&rdquo;>Estimating Shared Subspace with AJIVE: The Power and Limitation of Multiple Data Matrices</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>Annals of Statistics, 2021</i><br /></h2>
<ul>
<li><p><a href="Publication/RobustPCA/RPCA_noise.pdf" target=&ldquo;blank&rdquo;>Bridging Convex and Nonconvex Optimization in Robust PCA: Noise, Outliers, and Missing Data</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>Foundations and Trends&reg; in Machine Learning, 2021</i></h2>
<ul>
<li><p><a href="Publication/Spectral/SpectralMethods_FnTarticle-nowplain.pdf" target=&ldquo;blank&rdquo;>Spectral Methods for Data Science: A Statistical Perspective</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>Foundations of Computational Mathematics, 2020</i> <br /></h2>
<ul>
<li><p><a href="Publication/Implicit/paper.pdf" target=&ldquo;blank&rdquo;>Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>IEEE Transactions on Information Theory, 2021</i><br /></h2>
<ul>
<li><p><a href="Publication/Mixture_LowRank/mixed-matrix-sensing.pdf" target=&ldquo;blank&rdquo;>Learning Mixtures of Low-Rank Models</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>IEEE Transactions on Information Theory, 2023</i>  <br /></h2>
<ul>
<li><p><a href="Publication/SqrtMC/SqrtMC.pdf" target=&ldquo;blank&rdquo;>Optimal Tuning-Free Convex Relaxation for Noisy Matrix Completion</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>IEEE Transactions on Signal Processing, 2021</i> <br /></h2>
<ul>
<li><p><a href="Publication/BalancingFree/RectangularMF_TSP.pdf" target=&ldquo;blank&rdquo;>Beyond Procrustes: Balancing-Free Gradient Descent for Asymmetric Low-Rank Matrix Sensing</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>Mathematical Programming, 2019</i></h2>
<ul>
<li><p><a href="Publication/RandomInit/main.pdf" target=&ldquo;blank&rdquo;>Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>NeurIPS, 2023</i> <a href="https://github.com/yugjerry/conf-mc" target=&ldquo;blank&rdquo;>[Code]</a> <br /></h2>
<ul>
<li><p><a href="Publication/CMC/cmc.pdf" target=&ldquo;blank&rdquo;>Conformalized Matrix Completion</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>Proceedings of the National Academy of Sciences, 2019</i></h2>
<ul>
<li><p><a href="Publication/MC_inference/MC_inference.pdf" target=&ldquo;blank&rdquo;>Inference and Uncertainty Quantification for Noisy Matrix Completion</a> 
<i></i>
</p>
</li>
</ul>
<h2>Low-rank matrix recovery <i>SIAM Journal on Optimization, 2020</i></h2>
<ul>
<li><p><a href="Publication/NoisyMC/NoisyMC.pdf" target=&ldquo;blank&rdquo;>Noisy Matrix Completion: Understanding Statistical Guarantees for Convex Relaxation via Nonconvex Optimization</a> 
<i></i>
</p>
</li>
</ul>
<h2>Others</h2>
<ul>
<li><p><a href="Publication/SSL/ssl.pdf" target=&ldquo;blank&rdquo;>Unraveling Projection Heads in Contrastive Learning: Insights from Expansion and Shrinkage</a> 
<i></i>
</p>
</li>
</ul>
<h2>Others <i>ACM Transactions on Information Systems, 2017</i> <br /></h2>
<ul>
<li><p><a href="Publication/Panther/TOIS.pdf" target=&ldquo;blank&rdquo;>Fast and Flexible Top-\(K\) Similarity Search on Large Networks</a> 
<i></i>
</p>
</li>
</ul>
<h2>Others <i>Journal of the American Statistical Association, 2020</i></h2>
<ul>
<li><p><a href="Publication/ISA/jasa.pdf" target=&ldquo;blank&rdquo;>Inter-Subject Analysis: A Partial Gaussian Graphical Model Approach</a> 
<i></i>
</p>
</li>
</ul>
<h2>Others <i>Statistical Science, 2020</i> <br /></h2>
<ul>
<li><p><a href="Publication/DL_survey/DL_survey.pdf" target=&ldquo;blank&rdquo;>A Selective Overview of Deep Learning</a> 
<i></i>
</p>
</li>
</ul>
<h2>Ranking</h2>
<ul>
<li><p><a href="Publication/Rasch/Rasch.pdf" target=&ldquo;blank&rdquo;>Random Pairing MLE for Estimation of Item Parameters in Rasch Model</a> 
<i></i>
</p>
</li>
</ul>
<h2>Ranking <i>COLT, 2024</i> <a href="Publication/RankingSemiRandom/topK_semi_random_slides.pdf" target=&ldquo;blank&rdquo;>[Slides]</a></h2>
<ul>
<li><p><a href="Publication/RankingSemiRandom/Semi_Random_Ranking.pdf" target=&ldquo;blank&rdquo;>Top-\(K\) Ranking with a Monotone Adversary</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>ICLR, 2023</i> <br /></h2>
<ul>
<li><p><a href="Publication/OFTRL/OFTRL.pdf" target=&ldquo;blank&rdquo;>\(O(1/T)\) Convergence of Optimistic-Follow-the-Regularized-Leader in Two-Player Zero-Sum Markov Games</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>ICML, 2023</i></h2>
<ul>
<li><p><a href="Publication/JSRL/jsrl.pdf" target=&ldquo;blank&rdquo;>Jump-Start Reinforcement Learning</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>IEEE Transactions on Information Theory, 2022</i>  <br /></h2>
<ul>
<li><p><a href="Publication/OfflineRL/OfflineRL.pdf" target=&ldquo;blank&rdquo;>Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>IEEE Transactions on Information Theory, 2022</i> <a href="Publication/OPE/OPE_slides.pdf" target=&ldquo;blank&rdquo;>[Slides]</a> <br /></h2>
<ul>
<li><p><a href="Publication/OPE/OPE.pdf" target=&ldquo;blank&rdquo;>Minimax Off-Policy Evaluation for Multi-Armed Bandits</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>IEEE Transactions on Information Theory, 2024</i>  <br /></h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.19001" target=&ldquo;blank&rdquo;>High-Probability Sample Complexities for Policy Evaluation with Linear Function Approximation</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>NeurIPS, 2022</i>  <a href="Publication/ContextualBandit/lcb_bounds_poster.pdf" target=&ldquo;blank&rdquo;>[Poster]</a> <br /></h2>
<ul>
<li><p><a href="Publication/ContextualBandit/lcb_bounds_arxiv.pdf" target=&ldquo;blank&rdquo;>Pessimism for Offline Linear Contextual Bandits using \(\ell_{p}\) Confidence Sets</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <i>Neurips, 2024</i></h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2411.12786" target=&ldquo;blank&rdquo;>Off-Policy Estimation with Adaptively Collected Data: The Power of Online Learning</a> 
<i></i>
</p>
</li>
</ul>
<h2>Reinforcement learning <a href="Publication/BatchedBandit/batch_nb_slides.pdf" target=&ldquo;blank&rdquo;>[Slides]</a><br /></h2>
<ul>
<li><p><a href="Publication/BatchedBandit/BatchedNB.pdf" target=&ldquo;blank&rdquo;>Batched Nonparametric Contextual Bandits</a> 
<i></i>
</p>
</li>
</ul>
<h2>Scaled gradient descent <i>ICML, 2023</i> <a href="Publication/Overparam/ScaledGD_Overparam_Purdue.pdf" target=&ldquo;blank&rdquo;>[Slides]</a><br /></h2>
<ul>
<li><p><a href="Publication/Overparam/ScaledGD_Overparam.pdf" target=&ldquo;blank&rdquo;>The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing</a> 
<i></i>
</p>
</li>
</ul>
<h2>Scaled gradient descent <i>IEEE Transactions on Signal Processing, 2021</i><br /></h2>
<ul>
<li><p><a href="Publication/ScaledSM/ScaledNonsmooth.pdf" target=&ldquo;blank&rdquo;>Low-Rank Matrix Recovery with Scaled Subgradient Methods: Fast and Robust Convergence Without the Condition Number</a> 
<i></i>
</p>
</li>
</ul>
<h2>Scaled gradient descent <i>Information and Inference: A Journal of the IMA, 2023</i></h2>
<ul>
<li><p><a href="Publication/TPCA/TPCA.pdf" target=&ldquo;blank&rdquo;>Fast and Provable Tensor Robust Principal Component Analysis via Scaled Gradient Descent</a> 
<i></i>
</p>
</li>
</ul>
<h2>Scaled gradient descent <i>Journal of Machine Learning Research, 2021</i><br /></h2>
<ul>
<li><p><a href="Publication/ScaledGD/ScaledGD.pdf" target=&ldquo;blank&rdquo;>Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent</a> 
<i></i>
</p>
</li>
</ul>
<h2>Scaled gradient descent <i>Journal of Machine Learning Research, 2022</i><br /></h2>
<ul>
<li><p><a href="Publication/Tensor/Tensor_ScaledGD.pdf" target=&ldquo;blank&rdquo;>Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements</a> 
<i></i>
</p>
</li>
</ul>
<h2>Transfer learning</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2411.15624" target=&ldquo;blank&rdquo;>Trans-Glasso: A Transfer Learning Approach to Precision Matrix Estimation</a> 
<i></i>
</p>
</li>
</ul>
<ul>
<li><p><a href="Publication/DRL/isoDRL.pdf" target=&ldquo;blank&rdquo;>Distributionally Robust Risk Evaluation with an Isotonic Constraint</a> 
<i></i>
</p>
</li>
</ul>
<h2>Transfer learning <i>ICML, 2022</i> (long presentation)</h2>
<ul>
<li><p><a href="Publication/CovariateShiftNW/main.pdf" target=&ldquo;blank&rdquo;>A New Similarity Measure for Covariate Shift with Applications to Nonparametric Regression</a> 
<i></i>
</p>
</li>
</ul>
<h2>Transfer learning <a href="https://github.com/reesepathak/lowerlassosim" target=&ldquo;blank&rdquo;>[Code</a>]</h2>
<ul>
<li><p><a href="Publication/Lasso/lasso_suboptimality.pdf" target=&ldquo;blank&rdquo;>On the Design-Dependent Suboptimality of the Lasso</a> 
<i></i>
</p>
</li>
</ul>
<h2>Unknown</h2>
<ul>
<li><p><a href="Publication/Covariate-shift-MLE/CS-MLE.pdf" target=&ldquo;blank&rdquo;>Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift</a> 
<i></i>
</p>
</li>
</ul>
<ul>
<li><p><a href="Publication/CovariateShiftKernel/CovariateShift.pdf" target=&ldquo;blank&rdquo;>Optimally Tackling Covariate Shift in RKHS-Based Nonparametric Regression</a> 
<i></i>
</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-02-14 13:54:11 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
