\pdfminorversion=4
\documentclass[compress,
mathserif,wide,%red,
%handout
]{beamer}

\input{../StyleFiles/lec_style}

\graphicspath{{../../../Figures/}}


\title % (optional, use only with long paper titles)
{Spectral methods: $\ell_2$ perturbation theory}

\defbeamertemplate*{title page}{customized}[1][]
{

  \hfill {\em \courseTitle}

  \begin{center}
    \vspace{2.5em}
    \usebeamerfont{title} {\Large\bf\inserttitle} \par
  
    \vspace{1.5em}
    \includegraphics[width=2cm]{\LectureFigs/UC_logo.png} 
  
    \vspace{1em}
    {\large Cong Ma \par }

    \vspace{0.2em}
    { \large \quad University of Chicago, Autumn 2021 }
  \end{center}

  \vfill
}

\setcounter{subsection}{3}

\begin{document}


\begin{frame}[plain]
  \titlepage

\end{frame}

\begin{frame}
	\frametitle{Matrix perturbation theory (spectral analysis)}
	Let $\bm{M}^{\star}$ be a ``simple'' matrix, and $\bm{E}$ be a perturbation matrix
	
	{\hfill \footnotesize \em --- ``simple'' means spectral structure of $\bm{M}^{\star}$ is understood} 
	
	\vfill
	{
\setbeamercolor{block body}{bg=babyblueeyes,fg=black}

\begin{varblock}[\textwidth]{}
{\bf Goal of matrix perturbation theory: } \\
\begin{center}
Understand how eigenspaces (resp.~eigenvalues) / singular subspaces (resp.~singular values) of $\bm{M}^{\star} + \bm{E}$ change w.r.t.~perturbation $\bm{E}$
\end{center}
\end{varblock}
}
\end{frame}


\begin{frame}
\frametitle{Outline}

\begin{itemize}
  \itemsep1em
  \item Preliminaries: basic matrix analysis
  \item Distance between two subspaces
  \item Eigenspace perturbation theory
  \item Perturbation bounds for singular subspaces
  \item Eigenvector perturbation bounds for probability transition matrices
\end{itemize}

\end{frame}



\begin{frame}[plain]

\vfill
\begin{center}
  {\Large\bf Basic matrix analysis}

\end{center}
%\vfill
\vfill

\end{frame}


\begin{frame}
	\frametitle{Unitarily invariant norms}
	\begin{definition}
A matrix norm $\vertiii{\cdot}$ on $\mathbb{R}^{m\times n}$ is said to be unitarily invariant if
%
\[
	\vertiii{\bm{A}}=\vertiiibig{\bm{U}^{\top}\bm{A}\bm{V}}
\]
%
holds for any matrix $\bm{A}\in\mathbb{R}^{m\times n}$ and any two square orthonormal
matrices $\bm{U}\in\mathcal{O}^{m\times m}$ and $\bm{V}\in\mathcal{O}^{n\times n}$.
\end{definition}

\vfill

Examples: 
	\begin{itemize}
		% \itemsep1em
		\item $\|\bm{A}\|$: spectral norm (largest singular value of $\bm{A}$)
	
		\item $\|\bm{A}\|_{\mathrm{F}}$: Frobenius norm ($\|\bm{A}\|_{\mathrm{F}}= \sqrt{\mathsf{tr}(\bm{A}^{\top}\bm{A})}=\sqrt{\sum_{i,j}A_{i,j}^2}$)
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Properties of unitarily invariant norms}
\begin{lemma}
\label{prop:unitary_norm_relation}
For any unitarily invariant norm $\vertiii{\cdot}$, one has 
%
\begin{align*}
\vertiii{\bm{A}\bm{B}} & \leq\vertiii{\bm{A}} \cdot  \left\Vert \bm{B}\right\Vert, &  & \vertiii{\bm{A}\bm{B}}\leq\vertiii{\bm{B}} \cdot \left\Vert \bm{A}\right\Vert; \\
\vertiii{\bm{A}\bm{B}} & \geq\vertiii{\bm{A}} \, \sigma_{\min}\left(\bm{B}\right), &  & \text{if }\bm{B} \text{ is square}; \\
\vertiii{\bm{A}\bm{B}}& \geq\vertiii{\bm{B}} \, \sigma_{\min}\left(\bm{A}\right), & & \text{if }\bm{A} \text{ is square}. 
\end{align*}
%
%where $\sigma_{\min}(\bm{A})$ denotes the smallest singular value of a matrix $\bm{A}$. 
\end{lemma}

\vfill
{\footnotesize Exercise: prove this lemma for special cases $\|\cdot\|$ and $\|\cdot\|_{\mathrm{F}}$}
\end{frame}

\begin{frame}
	\frametitle{Eigenvalue perturbation bounds}
	\begin{lemma}[Weyl's inequality for eigenvalues]
\label{lemma:weyl}
Let $\bm{A},\bm{E}\in\mathbb{R}^{n\times n}$ be two real symmetric matrices.
	For every $1\leq i\leq n$, the $i$-th largest eigenvalues of $\bm{A}$ and $\bm{A}+\bm{E}$ obey
%
\begin{equation*}
	\left|\lambda_{i}\left(\bm{A}\right)-\lambda_{i}\left(\bm{A} +\bm{E}\right)\right|\leq\left\Vert \bm{E}\right\Vert .
\end{equation*}
%
\end{lemma}
\uncover<3->{{\hfill \em \footnotesize --- proof left as exercise}}

\pause
\vfill
{
\setbeamercolor{block body}{bg=babyblueeyes,fg=black}

\begin{varblock}[\textwidth]{}
\centering
eigenvalues of real symmetric matrices are stable against perturbations
\end{varblock}
}



\end{frame}

\begin{frame}
	\frametitle{Singular value perturbation bounds}
	\begin{lemma}[Weyl's inequality for singular values]
\label{lemma:weyls-singular-value}
Let $\bm{A},\bm{E}\in\mathbb{R}^{m\times n}$ be two general matrices.
Then for every $1\leq i\leq \min\{m,n\}$, the $i$-th largest singular values of $\bm{A}$ and $\bm{A}+\bm{E}$ obey 
%
\[
	\left|\sigma_{i}\left(\bm{A}+\bm{E}\right)-\sigma_{i}\left(\bm{A}\right)\right|\leq\left\Vert \bm{E}\right\Vert .
\]
%
\end{lemma}

\vfill
{
\setbeamercolor{block body}{bg=babyblueeyes,fg=black}

\begin{varblock}[\textwidth]{}
\centering
singular values are stable against perturbations
\end{varblock}
}
\end{frame}


\begin{frame}
	\frametitle{Proof of Lemma~\ref{lemma:weyls-singular-value}}
We begin with introducing a useful ``dilation" trick: 
	\begin{definition}[Symmetric dilation]
\label{defn:sym-dilation}
For $\bm{A}\in\mathbb{R}^{n_{1}\times n_{2}}$, define its symmetric dilation $\mathcal{S}(\bm{A})$ to be 
%
\[
\mathcal{S}(\bm{A})=\left[\begin{array}{cc}
\bm{0} & \bm{A}\\
\bm{A}^{\top} & \bm{0}
\end{array}\right] \in \mathbb{R}^{(n_1 + n_2) \times (n_1 + n_2)}.
\]
%
 \end{definition}
 
 
 Then one has the following eigendecomposition
for $\mathcal{S}(\bm{A})$: 
\begin{equation*}
\mathcal{S}(\bm{A})=\frac{1}{\sqrt{2}}\left[\begin{array}{cc}
\bm{U} & \bm{U}\\
\bm{V} & -\bm{V}
\end{array}\right]\cdot\left[\begin{array}{cc}
\bm{\Sigma} & \bm{0}\\
\bm{0} & -\bm{\Sigma}
\end{array}\right]\cdot\frac{1}{\sqrt{2}}\left[\begin{array}{cc}
\bm{U} & \bm{U}\\
\bm{V} & -\bm{V}
\end{array}\right]^{\top}.
\end{equation*}
Two observations: for $1\leq i\leq \min\{m,n\}$, $\lambda_{i}(\mathcal{S}(\bm{A})) = \sigma_{i}(\bm{A})$, and $\|\mathcal{S}(\bm{A})\| = \|\bm{A}\|$. Apply Lemma~\ref{lemma:weyl} to finish the proof.
\end{frame}



\begin{frame}[plain]

\vfill
\begin{center}
  {\Large\bf Distance between two subspaces}
\end{center}
%\vfill
\vfill

\end{frame}

\begin{frame}
	\frametitle{Setup and notation}
	\begin{itemize}
		\item Two $r$-dimensional subspaces $\mathcal{U}^{\star}$ and $\mathcal{U}$ in $\mathbb{R}^{n}$
		\item Two orthonormal matrices $\bm{U}^{\star}$ and $\bm{U}$ in $\mathbb{R}^{n \times r}$
		\item Orthogonal complements: $[\bm{U}^\star, \bm{U}^\star_{\perp}]$, and $[\bm{U}, \bm{U}_{\perp}]$
	\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Question: how to measure distance?}

	\begin{itemize}
		\item  $\|\bm{U}-\bm{U}^\star\|_{\mathrm{F}}$ and $\|\bm{U}-\bm{U}^\star\|$ are not appropriate,  since they fall short of accounting for 
			\hspace{-3.5em} $\underset{\alertb{\forall\text{ orthonormal } \bm{R}\in \mathbb{R}^{r\times r}, ~\bm{U} \text{ and } \bm{U}\bm{R} \text{ represent same subspace}}}{\underbrace{\text{global orthonormal transformation}}}$ 
	
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Three valid choices of distance}
	\begin{itemize}
		\item Distance modulo \emph{optimal rotation}
		\item Distance using \emph{projection matrices}
		\item Geometric construction via \emph{principal/canonical angles}
	\end{itemize}
\end{frame}


\begin{frame}
	\frametitle{Distance modulo optimal rotation}
	Given global rotation ambiguity, it is natural to adjust for rotation before computing distance:
	\begin{align*}
	\mathsf{dist}_{\vertiii{\cdot}}\big(\bm{U},{\bm{U}}^{\star}\big) \coloneqq
	\min _{\bm{R}\in \mathcal{O}^{r\times r}} \vertiiibig{ \bm{U} \bm{R} - \bm{U}^{\star}  } 
\end{align*}
\end{frame}

\begin{frame}
	\frametitle{Distance using projection matrices}
	Key observation: projection matrix $\bm{U} \bm{U}^{\top}$ associated with subspace $\mathcal{U}$ is unique

	\begin{equation*}
	\mathsf{dist}_{\mathsf{p},\vertiii{\cdot}}\big(\bm{U},{\bm{U}}^{\star}\big) \coloneqq \vertiiibig{\bm{U} \bm{U}^{\top}- {\bm{U}}^{\star} {\bm{U}}^{\star\top}}
\end{equation*}
\end{frame}


\begin{frame}
\frametitle{Principal angles between two eigen-spaces}

\smallskip

In addition to ``distance'', one might also be interested in ``angles''


\begin{columns}
	\begin{column}{0.2\textwidth}
	\end{column}
	
	\begin{column}{0.3\textwidth}
		\begin{center}
			\includegraphics[width=0.95\textwidth]{principal_angle_lines.pdf}
		\end{center}
	\end{column}

	\begin{column}{0.1\textwidth}
	\end{column}


	\begin{column}{0.3\textwidth}
		\begin{center}
			\includegraphics[width=0.8\textwidth]{principal_angle.pdf}
		\end{center}
	\end{column}

	\begin{column}{0.2\textwidth}
	\end{column}

\end{columns}


\bigskip

\vfill



We can quantify the similarity between two lines (represented resp.~by unit vectors $\bm{u}$ and $\bm{u}^\star$) by an angle between them  
%
\[
	\theta = \arccos \langle \bm{u}, \bm{u}^\star \rangle
\]


\end{frame}





\begin{frame}
\frametitle{Principal angles between two eigen-spaces}


More generally, for $r$-dimensional subspaces, one needs $r$ angles

\bigskip
\bigskip


Specifically, given $\|\bm{U}^{\top}\bm{U}^{\star}\| \leq 1$, we  write the singular value decomposition (SVD) of $\bm{U}^{\top}\bm{U}^{\star} \in\mathbb{R}^{r\times r}$ as
%
\[
	\bm{U}^{\top}\bm{U}^{\star}
	=\bm{X} {\footnotesize  \underset{\alertb{\eqqcolon\cos \bm{\Theta}}}{\underbrace{ \left[\begin{array}{ccc}
		\cos\theta_{1}\\
 		& \ddots\\
 		&  & \cos\theta_{r}
	\end{array}\right]} }}
	\bm{Y}^{\top} \eqqcolon \bm{X}\cos\bm{\Theta}\,\bm{Y}^{\top}
\]
%
where $\{\theta_{1},\ldots,\theta_{r}\}$ are called the \alert{principal angles}
between $\bm{U}$ and $\bm{U}^\star$

\end{frame}


\begin{frame}
	\frametitle{Distance using principal angles}
With principal angles in place, we can define $\sin\bm{\Theta}$ distance between subspaces as
	\begin{equation*}
	\mathsf{dist}_{\mathsf{sin},\vertiii{\cdot}}\big(\bm{U}, {\bm{U}}^{\star} \big) \coloneqq \vertiii{\sin\bm{\Theta}}
\end{equation*}
%%
where
%
\begin{equation*}
	\bm{\Theta}\coloneqq {\footnotesize \left[\begin{array}{ccc}
\theta_{1}\\
 & \ddots\\
 &  & \theta_{r}
	\end{array}\right] }
	,\quad
	\sin\bm{\Theta}\coloneqq { \left[\begin{array}{ccc}
\sin\theta_{1}\\
 & \ddots\\
 &  & \sin\theta_{r}
	\end{array}\right] } 
	\label{eq:defn-Theta-sin-Theta}
\end{equation*}
\end{frame}


\begin{frame}
	\frametitle{Link between projections and principal angles}
	\begin{lemma}\label{lemma:link-projections-angles}
The following identities are true:
\begin{subequations}
\begin{align*}
	\big\Vert \bm{U}\bm{U}^{\top}-{\bm{U}}^{\star}{\bm{U}}^{\star\top}\big\Vert
	& =\left\Vert \sin\bm{\Theta}\right\Vert  = \big\Vert \bm{U}_{\perp}^{\top}\bm{U}^{\star}\big\Vert = \big\Vert \bm{U}^{\top}\bm{U}_{\perp}^{\star}\big\Vert ; \\
	\tfrac{1}{\sqrt{2}} \big\Vert \bm{U}\bm{U}^{\top}-{\bm{U}}^{\star}{\bm{U}}^{\star\top}\big\Vert _{\mathrm{F}}
	& =\left\Vert \sin\bm{\Theta}\right\Vert _{\mathrm{F}}
	=  \big\Vert \bm{U}_{\perp}^{\top}\bm{U}^{\star}\big\Vert_{\mathrm{F}} =   \big\Vert \bm{U}^{\top}\bm{U}_{\perp}^{\star} \big\Vert_{\mathrm{F}} .
\end{align*}
\end{subequations}
\end{lemma}

\vfill 
\begin{itemize}
	\item sanity check: if $\bm{U} = \bm{U}^\star$, then  everything is 0
\end{itemize}
%
\end{frame}





\begin{frame}
\frametitle{Proof of Lemma~\ref{lemma:link-projections-angles}}

We prove the claim for spectral norm; the claim for Frobenius norm follows similar argument. Note that
\begin{align*}
\|\bm{U}^{\top}\bm{U}^\star_{\perp}\| & = \big\| \bm{U}^{\top}\underset{\alertb{=\bm{I}-\bm{U}^\star\bm{U}^{\star\top}}}{\underbrace{\bm{U}^\star_{\perp}\bm{U}^{\star\top}_{\perp}}}\bm{U} \big\|^{\frac{1}{2}}\\
 & = \big\| \bm{U}^{\top}\bm{U}-\bm{U}^{\top}\bm{U}^\star\bm{U}^{\star\top}\bm{U} \big \|^{\frac{1}{2}}\\
 & = \big\| \bm{I}-\bm{X}\cos^{2}\bm{\Theta} \,\bm{X}^{\top} \big\|^{\frac{1}{2}}\qquad \alertb{(\text{write }\bm{U}^{\top}\bm{U}^\star=\bm{X}\cos\bm{\Theta}\,\bm{Y}^{\top})}\\
 & = \big\|\bm{I}-\cos^{2}\bm{\Theta} \big\|^{\frac{1}{2}} \\
 & = \|\sin^{2}\bm{\Theta}\|^{\frac{1}{2}} \\
 & =\|\sin\bm{\Theta}\|
\end{align*}

\end{frame}


\begin{frame}
\frametitle{Proof of Lemma~\ref{lemma:link-projections-angles} (cont.)}
Given that singular values are unitarily invariant, it suffices to look at
the singular values of the following matrix
%
\begin{align*}
\left[\begin{array}{c}
\bm{U}^{\top}\\
\bm{U}_{\perp}^{\top}
\end{array}\right]\big(\bm{U}\bm{U}^{\top}- {\bm{U}}^{\star} {\bm{U}}^{\star\top}\big)
	\big[
{\bm{U}}_{\perp}^{\star} , {\bm{U}}^{\star} \big]=\left[\begin{array}{cc}
	\bm{U}^{\top}{\bm{U}}_{\perp}^{\star} & \bm{0}\\
\bm{0} & -\bm{U}_{\perp}^{\top} {\bm{U}}^{\star}
\end{array}\right]
\end{align*}

which further implies
\begin{subequations}
\begin{align*}
	\big\|\bm{U}\bm{U}^{\top}-\bm{U}^{\star}\bm{U}^{\star\top}\big\| &= \max\big\{\big\|\bm{U}^{\top}\bm{U}_{\perp}^{\star}\big\|,\big\|\bm{U}_{\perp}^{\top}\bm{U}^{\star}\big\|\big\}; \\
	\big\|\bm{U}\bm{U}^{\top}-\bm{U}^{\star}\bm{U}^{\star\top}\big\|_{\mathrm{F}} & =\Big(\big\|\bm{U}^{\top}\bm{U}_{\perp}^{\star}\big\|_{\mathrm{F}}^{2}+\big\|\bm{U}_{\perp}^{\top}\bm{U}^{\star}\big\|_{\mathrm{F}}^{2}\Big)^{1/2}
\end{align*}
\end{subequations}

\end{frame}




\begin{frame}
	\frametitle{Link between optimal rotations and projections}
	\begin{lemma}\label{prop:rotation-UR}
	The following identities are true:
\begin{align*}
	\|\bm{U}\bm{U}^{\top} - \bm{U}^{\star}\bm{U}^{\star\top} \|
	&\leq
	\min_{\bm{R}\in \mathcal{O}^{r\times r}}\big\|\bm{U}\bm{R}-\bm{U}^{\star}\big\|
	%\mathsf{dist} \big(\bm{U},\bm{U}^{\star}\big)
	\leq \sqrt{2} \|\bm{U}\bm{U}^{\top} - \bm{U}^{\star}\bm{U}^{\star\top} \|; \\
	\tfrac{1}{\sqrt{2}} \|\bm{U}\bm{U}^{\top} - \bm{U}^{\star}\bm{U}^{\star\top} \|_{\mathrm{F}}
	%\mathsf{dist}_{\mathrm{F}} \big(\bm{U},\bm{U}^{\star}\big)
	&\leq
	\min_{\bm{R}\in\mathcal{O}^{r\times r}}\left\Vert \bm{U}\bm{R}-\bm{U}^{\star}\right\Vert _{\mathrm{F}}
	%\mathsf{dist}_{\mathrm{F}} \big(\bm{U},\bm{U}^{\star}\big)
	\leq \|\bm{U}\bm{U}^{\top} - \bm{U}^{\star}\bm{U}^{\star\top} \|_{\mathrm{F}}.
	%\mathsf{dist} \big(\bm{U},\bm{U}^{\star}\big).
	%\label{eq:equiv-rotation-dist-spec}
\end{align*}
%\end{subequations}
\end{lemma}

{\hfill \footnotesize \em --- proof left as exercise}
\end{frame}


\begin{frame}
	\frametitle{Summary of distance metrics}
So far we have discussed
	\begin{align}
	\mathrm{1)} \quad & \vertiiibig{\bm{U}\bm{U}^{\top} - \bm{U}^{\star}\bm{U}^{\star\top}}  \notag\\
	\mathrm{2)} \quad & \vertiiibig{\sin\bm{\Theta}}  \notag\\
	\mathrm{3)} \quad & \vertiiibig{ \bm{U}_{\perp}^{\top}\bm{U}^{\star} }=\vertiiibig{ \bm{U}^{\top}\bm{U}^{\star}_{\perp} }  \notag \\
	\mathrm{4)} \quad & \min_{\bm{R}\in \mathcal{O}^{r\times r}} \vertiiibig{ \bm{U}\bm{R} - \bm{U}^{\star} } \notag
\end{align}

\pause
Our choice of distance: 
\begin{subequations}
\label{eq:dist_UUstar}
\begin{align*}
	\mathsf{dist}(\bm{U},\bm{U}^{\star}) &\coloneqq \min_{\bm{R}\in \mathcal{O}^{r\times r}} \big\| \bm{U}\bm{R} - \bm{U}^{\star} \big\|; \\
	%\big\| \bm{U} \bm{U}^{\top}  -  \bm{U}^{\star} \bm{U}^{\star\top} \big\|, \\
	\mathsf{dist}_{\mathrm{F}}(\bm{U},\bm{U}^{\star}) &\coloneqq  \min_{\bm{R}\in \mathcal{O}^{r\times r}} \big\| \bm{U}\bm{R} - \bm{U}^{\star} \big\|_{\mathrm{F}}
	%\big\| \bm{U} \bm{U}^{\top}  -  \bm{U}^{\star} \bm{U}^{\star\top} \big\|_{\mathrm{F}},
\end{align*}
\end{subequations}

\end{frame}



\begin{frame}[plain]

\vfill
\begin{center}
  {\Large\bf Eigenspace perturbation theory}
\end{center}
%\vfill
\vfill

\end{frame}



\begin{frame}
\frametitle{Setup and notation}
	\label{frame:setup-spectral}

Consider 2 symmetric matrices $\bm{M}^{\star}$, $\bm{M}=\bm{M}^{\star}+\bm{E}\in \mathbb{R}^{n\times n}$ with  eigen-decompositions 
\begin{alignat*}{2}
\bm{M}^{\star} & =\sum_{i=1}^{n}\lambda_{i}^{\star}\bm{u}_{i}^{\star}\bm{u}_{i}^{\star\top} & & =\left[\begin{array}{cc}
\bm{U}^{\star} & \bm{U}_{\perp}^{\star}\end{array}\right]\left[\begin{array}{cc}
\bm{\Lambda}^{\star} & \bm{0}\\
\bm{0} & \bm{\Lambda}_{\perp}^{\star}
\end{array}\right]\left[\begin{array}{c}
\bm{U}^{\star\top}\\
\bm{U}_{\perp}^{\star\top}
\end{array}\right];   \\
\bm{M} & =\sum_{i=1}^{n}\lambda_{i}\bm{u}_{i}\bm{u}_{i}^{\top} & & =\left[\begin{array}{cc}
\bm{U} & \bm{U}_{\perp}\end{array}\right]\left[\begin{array}{cc}
\bm{\Lambda} & \bm{0}\\
\bm{0} & \bm{\Lambda}_{\perp}
\end{array}\right]\left[\begin{array}{c}
\bm{U}^{\top}\\
\bm{U}_{\perp}^{\top}
\end{array}\right]
\end{alignat*}

\end{frame}




\begin{frame}
\frametitle{Setup and notation}


% Preview source code for paragraph 1

{\small
\begin{align*}
\bm{M} & =\Big[\underset{\alertb{\bm{U}}}{\underbrace{\begin{array}{ccc}
\bm{u}_{1} & \cdots & \bm{u}_{r}\end{array}}}\underset{\alertb{\bm{U}_{\perp}}}{\underbrace{\begin{array}{ccc}
\bm{u}_{r+1} & \cdots & \bm{u}_{n}\end{array}}}\Big]\\
 & \qquad\cdot \left[\begin{array}{cc}
\underset{\alertb{\bm{\Lambda}}}{\underbrace{\begin{array}{ccc}
\lambda_{1}\\
 & \ddots\\
 &  & \lambda_{r}
\end{array}}}\\
 & \underset{\alertb{\bm{\Lambda}_{\perp}}}{\underbrace{\begin{array}{ccc}
\lambda_{r+1}\\
 & \ddots\\
 &  & \lambda_{n}
\end{array}}}
\end{array}\right]\left[\begin{array}{c}
\begin{array}{c}
\bm{u}_{1}^{\top}\\
\vdots\\
\bm{u}_{r}^{\top}
\end{array}\\
\begin{array}{c}
\\
\bm{u}_{r+1}^{\top}\\
\vdots\\
\bm{u}_{n}^{\top}
\end{array}
\end{array}\right]
\hspace{-1.7em}\begin{array}{c}
\left.\begin{array}{c}
\\
\\
\\
\\
\end{array}\right\} \alertb{\bm{U}^{\top}}\\
\left.\begin{array}{c}
\\
\\
\\
\\
\end{array}\right\} \alertb{\bm{U}_{\perp}^{\top}}
\end{array}
\end{align*}


}

\end{frame}


\begin{frame}
\frametitle{Davis-Kahan's $\sin\bm{\Theta}$ theorem: a simple case}

\vspace{-0.5em}

\begin{columns}
	\begin{column}{0.35\textwidth}
		\begin{center}
			\includegraphics[width=0.65\textwidth]{Chandler_Davis.jpg} \\
			{\footnotesize  Chandler Davis}
		\end{center}
	\end{column}
	\begin{column}{0.35\textwidth}
		\begin{center}
			\includegraphics[width=0.7\textwidth]{William_Kahan.jpg} \\
			{\footnotesize William Kahan}
		\end{center}
	\end{column}


\end{columns}

\begin{theorem}[Davis-Kahan's $\sin\bm{\Theta}$ theorem: simple version] \label{thm:sin-Theta}
Suppose $\bm{M}^\star \succeq \bm{0}$ and is rank-$r$. If $\|\bm{E}\| < (1 - 1/ \sqrt{2}) \lambda_{r}(\bm{M}^\star)$, then
\begin{subequations}
\label{eq:davis-kahan-conclusion-corollary}
\begin{align*}
\mathsf{dist}\big(\bm{U},\bm{U}^{\star}\big) & \leq\sqrt{2}\;\|\sin\bm{\Theta}\|\leq\frac{2 \big\|\bm{E}\bm{U}^{\star}\big\|}{\lambda_{r}(\bm{M}^\star)}\leq\frac{2\|\bm{E}\|}{\lambda_{r}(\bm{M}^\star)};\\
	\mathsf{dist}_{\mathrm{F}}\big(\bm{U},\bm{U}^{\star}\big) & \leq\sqrt{2}\;\|\sin\bm{\Theta}\|_{\mathrm{F}}\leq\frac{2 \big\|\bm{E}\bm{U}^{\star}\big\|_{\mathrm{F}}}{\lambda_{r}(\bm{M}^\star)}\leq\frac{2\sqrt{r}\|\bm{E}\|}{\lambda_{r}(\bm{M}^\star)}.
\end{align*}
\end{subequations}
%
\end{theorem}

\end{frame}

\begin{frame}
	\frametitle{Interpretations of Davis-Kahan's $\sin\Theta$ theorem}
	
{
\setbeamercolor{block body}{bg=babyblueeyes,fg=black}

\begin{varblock}[\textwidth]{}
Suppose $\bm{M}^\star \succeq \bm{0}$ and is rank-$r$. If $\|\bm{E}\| < (1 - 1/ \sqrt{2}) \lambda_{r}(\bm{M}^\star)$, then
\begin{subequations}
\label{eq:davis-kahan-conclusion-corollary}
\begin{align*}
\mathsf{dist}\big(\bm{U},\bm{U}^{\star}\big) & \leq\sqrt{2}\;\|\sin\bm{\Theta}\|\leq\frac{2 \big\|\bm{E}\bm{U}^{\star}\big\|}{\lambda_{r}(\bm{M}^\star)}\leq\frac{2\|\bm{E}\|}{\lambda_{r}(\bm{M}^\star)}.
\end{align*}
\end{subequations}
\end{varblock}
}


\vfill 
Remarks: 
	\begin{itemize}
	\itemsep0.5em
	\item Eigen-gap $\lambda_{r}(\bm{M}^{\star}) = \lambda_{r}(\bm{M}^{\star}) - \lambda_{r+1}(\bm{M}^{\star})$
	\item Perturbation size $\|\bm{E}\|$
	\item Signal-to-noise (SNR) ratio $\frac{\lambda_{r}(\bm{M}^\star)}{\|\bm{E}\|}$
	\item $\|\bm{E} \bm{U}^{\star} \|$ is sometimes useful; we will see benefit later
	\item Necessity of $\|\bm{E}\| \lesssim \lambda_{r}(\bm{M}^\star)$
\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{What happens when SNR is small?}
A toy example (with $0<\epsilon<1$)
	\[
\bm{M}^{\star}=\left[\begin{array}{cc}
1+\epsilon & 0\\
0 & 1-\epsilon
\end{array}\right],
~~
\bm{E}=\left[\begin{array}{cc}
-\epsilon & \epsilon\\
\epsilon & \epsilon
\end{array}\right],~~
\bm{M}=\left[\begin{array}{cc}
1 & \epsilon\\
\epsilon & 1
\end{array}\right]
\]

Leading eigenvectors of $\bm{M}^{\star}$ and $\bm{M}$ are given respectively by
%
\[
\bm{u}_{1}^{\star}=\left[\begin{array}{c}
1\\
0
\end{array}\right],
\qquad\text{and}\qquad
\bm{u}_{1}=\frac{1}{\sqrt{2}} \left[\begin{array}{c}
1 \\
1
\end{array}\right] 
\]
%
Consequently, we have
%
\begin{align*}
	\big\|\bm{u}_{1}\bm{u}_{1}^{\top} - \bm{u}_{1}^{\star}\bm{u}_{1}^{\star\top}\big\| = \frac{1}{\sqrt{2}}, 
	\quad \text{and} \quad
	%\mathsf{dist}_{\mathrm{F}}(\bm{u}_{1}, \bm{u}_{1}^{\star})
	\big\|\bm{u}_{1}\bm{u}_{1}^{\top} - \bm{u}_{1}^{\star}\bm{u}_{1}^{\star\top}\big\|_{\mathrm{F}}= 1
\end{align*}

{\hfill \small \em --- large regardless of size of $\epsilon$ or size of the perturbation $\|\bm{E}\|$}
\end{frame}

\begin{frame}
\frametitle{Proof of  Theorem \ref{thm:sin-Theta}}


	We intend to control $\bm{U}_{\perp}^{\top} \bm{U}^\star$ by studying their interactions through $\bm{E}$:	
\begin{align*}
	\bm{U}_{\perp}^{\top} \bm{E} \bU^\star=\bm{U}_{\perp}^{\top} (\bM - \bM^\star) \bU^\star = \bm{\Lambda}_{\perp}\bm{U}_{\perp}^{\top}\bm{U}^{\star}-\bm{U}_{\perp}^{\top}\bm{U}^{\star}\bm{\Lambda}^{\star},
\end{align*}
which together with triangle inequality implies
\begin{align} \label{eq:sinTheta-LB1}
\vertiiibig{\bm{U}_{\perp}^{\top}\bm{E}\bm{U}^\star} & \geq \vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}\bm{\Lambda}^{\star}} - \vertiiibig{\bm{\Lambda}_{\perp}\bm{U}_{\perp}^{\top}\bm{U}^{\star}} \nonumber \\
	& \geq  \sigma_{\min}(\bm{\Lambda}^\star)  \vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}}- \|\bm{\Lambda}_{\perp}\| \cdot \vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}}
\end{align}
%
	In view of Weyl's inequality, one has $\|\bm{\Lambda}_{\perp}\|\leq\|\bm{E}\|$. In addition, we have $ \sigma_{\min}(\bm{\Lambda}^\star) = \lambda_{r}(\bm{M}^\star)$. These combined with relation~\eqref{eq:sinTheta-LB1} give
%
\[
	\vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}}
	\leq\frac{\vertiiibig{\bm{U}_{\perp}^{\top}\bm{E}\bm{U}^\star} }{\lambda_{r}(\bm{M}^\star) - \|\bm{E}\|}
	\leq\frac{ \sqrt{2} \| \bm{U}_{\perp} \| \cdot \vertiii{  \bm{E} \bm{U}^{\star} }}{\lambda_{r}(\bm{M}^{\star})}
	= \frac{  \sqrt{2}  \vertiii{  \bm{E} \bm{U}^{\star} } }{\lambda_{r}(\bm{M}^{\star})}
\]

This together with Lemmas \ref{lemma:link-projections-angles}-\ref{prop:rotation-UR} completes the proof


\end{frame}

\begin{frame}
	\frametitle{Davis-Kahan's $\sin\bm{\Theta}$ theorem: general case}
	\vspace{-0.5em}
	{\hfill \footnotesize \em --- $\mathsf{eigenvalues}(\bm{A})$: set of eigenvalues of $\bm{A}$}
	\begin{theorem}[Davis-Kahan's sin$\bm{\Theta}$ theorem: general version]
\label{thm:davis-kahan}Assume that

\vspace{-2em}
\begin{subequations}
\label{eq:assumption-eigenvalues-DK}
\begin{align}
	\mathsf{eigenvalues}(\bm{\Lambda}^{\star}) &\subseteq (-\infty, \alpha-\Delta] \cup [\beta+\Delta, \infty) ; \\
	\mathsf{eigenvalues}(\bm{\Lambda}_{\perp}) &\subseteq [\alpha,\beta].
\end{align}
\end{subequations}
%
for some quantities $\alpha,\beta\in \mathbb{R}$ and eigengap $\Delta>0$. Then one has
\begin{subequations}
\begin{align*}
\mathsf{dist}\big(\bm{U},\bm{U}^{\star}\big) & \leq\sqrt{2}\|\sin\bm{\Theta}\|\leq\frac{\sqrt{2}\big\|\bm{E}\bm{U}^{\star}\big\|}{\Delta}\leq\frac{\sqrt{2}\|\bm{E}\|}{\Delta};\\
\mathsf{dist}_{\mathrm{F}}\big(\bm{U},\bm{U}^{\star}\big) & \leq\sqrt{2}\|\sin\bm{\Theta}\|_{\mathrm{F}}\leq\frac{\sqrt{2}\big\|\bm{E}\bm{U}^{\star}\big\|_{\mathrm{F}}}{\Delta}\leq\frac{\sqrt{2r}\|\bm{E}\|}{\Delta}.
\end{align*}
\end{subequations}
%
\end{theorem}

\hfill \small \em --- conclusion remains valid if Assumption~\eqref{eq:assumption-eigenvalues-DK} is reversed
%

%

\end{frame}



\begin{frame}[plain]

\vfill
\begin{center}
  {\Large\bf Perturbation theory for singular subspaces}
\end{center}
%\vfill
\vfill

\end{frame}



\begin{frame}
\frametitle{Singular value decomposition}

Let $\bm{M}^{\star}$ and $\bm{M}=\bm{M}^{\star}+\bm{E}$ be two matrices
in $\mathbb{R}^{n_1 \times n_2}$ (WLOG, we assume $n_1\leq n_2$), whose SVDs
are given respectively by
%
\begin{alignat*}{2}
\bm{M}^{\star} & =\sum_{i=1}^{n_1}\sigma_{i}^{\star}\bm{u}_{i}^{\star}\bm{v}_{i}^{\star\top} & & =\left[\begin{array}{cc}
\bm{U}^{\star} & \bm{U}_{\perp}^{\star}\end{array}\right]\left[\begin{array}{ccc}
\bm{\Sigma}^{\star} & \bm{0} & \bm{0}\\
\bm{0} & \bm{\Sigma}_{\perp}^{\star} & \bm{0}
\end{array}\right]\left[\begin{array}{c}
\bm{V}^{\star\top}\\
\bm{V}_{\perp}^{\star\top}
\end{array}\right] \\
\bm{M} & =\sum_{i=1}^{n_1}\sigma_{i}\bm{u}_{i}\bm{v}_{i}^{\top} & & =\left[\begin{array}{cc}
\bm{U} & \bm{U}_{\perp}\end{array}\right]\left[\begin{array}{ccc}
\bm{\Sigma} & \bm{0} & \bm{0}\\
\bm{0} & \bm{\Sigma}_{\perp} & \bm{0}
\end{array}\right]\left[\begin{array}{c}
\bm{V}^{\top}\\
\bm{V}_{\perp}^{\top}
\end{array}\right]
\end{alignat*}


\begin{itemize}
	\item $\sigma_{1}\geq\cdots\geq\sigma_{n_1}$ (resp.~$\sigma_{1}^{\star}\geq\cdots\geq\sigma_{n_1}^{\star}$)
stand for the singular values of $\bm{M}$ (resp.~$\bm{M}^{\star}$)
arranged in descending order
	\item $\bm{U}, \bm{U}^{\star} \in \mathbb{R}^{n_1 \times r}$ have orthonormal columns
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Wedin's $\sin\bm{\Theta}$ theorem}


Davis-Kahan's theorem generalizes to singular subspace perturbation:

\vfill
\begin{theorem}[Wedin's sin$\bm{\Theta}$ theorem]
\label{thm:wedin} If $\|\bm{E}\|<\sigma_{r}^{\star}-\sigma_{r+1}^{\star}$, then one has
%
\begin{align*}
\max\left\{ \mathsf{dist}\big(\bm{U},\bm{U}^{\star}\big),\mathsf{dist}\big(\bm{V},\bm{V}^{\star}\big)\right\}
	& \leq\frac{ \sqrt{2} \max\big\{ \|\bm{E}^{\top}\bm{U}^{\star}\|,\|\bm{E}\bm{V}^{\star}\|\big\} }{\sigma_{r}^{\star}-\sigma_{r+1}^{\star}-\|\bm{E}\|};\\
\max\left\{ \mathsf{dist}_{\mathrm{F}}\big(\bm{U},\bm{U}^{\star}\big),\mathsf{dist}_{\mathrm{F}}\big(\bm{V},\bm{V}^{\star}\big)\right\}
	& \leq\frac{\sqrt{2}\max\big\{ \|\bm{E}^{\top}\bm{U}^{\star}\|_{\mathrm{F}},\|\bm{E}\bm{V}^{\star}\|_{\mathrm{F}}\big\} }{\sigma_{r}^{\star}-\sigma_{r+1}^{\star}-\|\bm{E}\|}
\end{align*}

\end{theorem}

\hfill \small \em --- can be simplified if $\|E\| < (1 - 1/\sqrt{2})(\sigma_{r}^{\star}-\sigma_{r+1}^{\star})$
\end{frame}



\begin{frame}
	\frametitle{Proof of Theorem~\ref{thm:wedin}}
	Similar to proof of Davis-Kahan theorem, we concentrate on $\bm{U}_{\perp}^{\top}\bm{U}^{\star}$ 
	\begin{align}
\bm{U}_{\perp}^{\top}\bm{U}^{\star}
	& =\bm{U}_{\perp}^{\top}\big(\bm{U}^{\star}\bm{\Sigma}^{\star}\bm{V}^{\star\top}\big)\bm{V}^{\star}\bm{\Sigma}^{\star-1}\nonumber \\
	& =\bm{U}_{\perp}^{\top}\left(\bm{M}-\bm{E}-\bm{U}_{\perp}^{\star}\bm{\Sigma}_{\perp}^{\star}\bm{V}_{\perp}^{\star\top}\right)\bm{V}^{\star}\bm{\Sigma}^{\star-1}\nonumber \\
 & =\bm{U}_{\perp}^{\top}\left(\bm{U}\bm{\Sigma}\bm{V}^{\top}+\bm{U}_{\perp}\bm{\Sigma}_{\perp}\bm{V}_{\perp}^{\top}-\bm{E}-\bm{U}_{\perp}^{\star}\bm{\Sigma}_{\perp}^{\star}\bm{V}_{\perp}^{\star\top}\right)\bm{V}^{\star}\bm{\Sigma}^{\star-1}\nonumber \\
 & =\bm{\Sigma}_{\perp}\bm{V}_{\perp}^{\top}\bm{V}^{\star}\bm{\Sigma}^{\star-1}-\bm{U}_{\perp}^{\top}\bm{E}\bm{V}^{\star}\bm{\Sigma}^{\star-1} .
	\label{eq:wedin-identity}
\end{align}

Applying triangle inequality and Lemma~\ref{prop:unitary_norm_relation} to identity~\eqref{eq:wedin-identity}
yields
%
\begin{align}
\vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}}
	& \leq \|\bm{\Sigma}_{\perp}\| \cdot \vertiiibig{\bm{V}_{\perp}^{\top}\bm{V}^{\star}} \cdot \|\bm{\Sigma}^{\star-1}\|
	 +  \| \bm{U}_{\perp}^{\top}\|\cdot \vertiiibig{\bm{E}\bm{V}^{\star}} \cdot \|\bm{\Sigma}^{\star-1}\| \notag\\
	 & =  \sigma_{r+1} \cdot \vertiiibig{\bm{V}_{\perp}^{\top}\bm{V}^{\star}} \cdot \frac{1}{\sigma_r^{\star}}
	 + \vertiiibig{\bm{E}\bm{V}^{\star}} \cdot \frac{1}{\sigma_r^{\star}}
	   \notag\\
 & \leq\frac{\sigma_{r+1}^{\star}+\|\bm{E}\|}{\sigma_{r}^{\star}}\vertiiibig{\bm{V}_{\perp}^{\top}\bm{V}^{\star}}+\frac{\vertiii{\bm{E}\bm{V}^{\star}}}{\sigma_{r}^{\star}}
	\label{eq:Uperp-Ustar-UB}
\end{align}
\end{frame}



\begin{frame}
	\frametitle{Proof of Theorem~\ref{thm:wedin} (cont.)}
	
	Repeating the same argument yields
%
\begin{align}
\vertiiibig{\bm{V}_{\perp}^{\top}\bm{V}^{\star}}\leq\frac{\vertiiibig{\bm{E}^{\top}\bm{U}^{\star}}}{\sigma_{r}^{\star}}
	+\frac{\sigma_{r+1}^{\star}+\|\bm{E}\|}{\sigma_{r}^{\star}}\vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}}
	\label{eq:Vperp-Vstar-UB}
\end{align}


To finish up, combine inequalities \eqref{eq:Uperp-Ustar-UB} and \eqref{eq:Vperp-Vstar-UB} to obtain
%
\begin{align*}
 & \max\big\{ \vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}},\vertiiibig{\bm{V}_{\perp}^{\top}\bm{V}^{\star}}\big\}
  \leq \frac{  \max\big\{  \vertiiibig{\bm{E}^{\top}\bm{U}^{\star}},\vertiiibig{\bm{E}\bm{V}^{\star}}  \big\} }{\sigma_r^{\star}}  \\
	&	\qquad\qquad\qquad\qquad
	+ \frac{\sigma_{r+1}^{\star}+\|\bm{E}\|}{\sigma_{r}^{\star}} \max\big\{ \vertiiibig{\bm{U}_{\perp}^{\top}\bm{U}^{\star}},\vertiiibig{\bm{V}_{\perp}^{\top}\bm{V}^{\star}}\big\} .
\end{align*}
%
When $\|\bm{E}\|<\sigma_r^{\star} - \sigma_{r+1}^{\star}$, we can rearrange terms to obtain desired results 
\end{frame}

\begin{frame}
	\frametitle{Extensions of Wedin's theorem}
	\begin{itemize}
		\item Single rotation matrix: Wedin shows us existence of two unitary matrices $\bm{R}_{U}, \bm{R}_{V}$ such that 
		\begin{align*}
	\max\big\{ \|\bm{U}\bm{R}_{U}-\bm{U}^{\star}\|_{\mathrm{F}}, \|\bm{V}\bm{R}_{V}-\bm{V}^{\star}\|_{\mathrm{F}} \big\} \quad \text{is small}
\end{align*}

	\begin{itemize}
		\item Can actually take same unitary matrix (exercise; hint ``dilation'')
	\end{itemize}
	
		\vfill
		\item Separate bounds for left and right singular vectors: 
		\begin{itemize}
		\item Can treat $\bm{U}$ and $\bm{V}$ differently and obtain sharper bounds
		\item Useful when $n_1$ and $n_2$ are drastically different
	\end{itemize}
	\end{itemize}
\end{frame}


\begin{frame}[plain]

\vfill
\begin{center}
  {\Large\bf Eigenvector perturbation for probability transition matrices}
\end{center}
%\vfill
\vfill

\end{frame}


\begin{frame}
	\frametitle{Eigen-decomposition for  asymmetric matrices}

	Eigen-decomposition for asymmetric matrices is trickier: 
	\begin{itemize}
		\itemsep0.5em
		\item[{\color{black}1.}]  both eigenvalues and eigenvectors might be complex-valued
		\item[{\color{black}2.}]  eigenvectors might not be orthogonal to each other
	\end{itemize}


	\vfill

	This lecture focuses on a special case:  {\bf probability transition matrices}

\end{frame}



\begin{frame}
	\frametitle{Probability transition matrices}

\vspace{-0.5em}
\begin{center}
	\includegraphics[width=0.3\textwidth]{markov_chain.pdf}  
\end{center}

\vspace{-0.5em}
Consider a Markov chain $\{X_t\}_{t\geq 0}$ 

\begin{itemize}
	\itemsep0.5em
	\item $n$ states 
	\item transition probability $\mathbb{P}\{ X_{t+1}=j\mid X_t=i \} = P_{i,j}$
	\item transition matrix $\bm{P}=[P_{i,j}]_{1\leq i,j\leq n}$
\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{Stationary distribution}
	Recall $\bm{P}$ is probability transition matrix
	\begin{itemize}
		\item $\bm{\pi}=[\pi_i]_{1\leq i\leq n}$ is stationary distribution of $\bm{P}$ if 
		\begin{equation*}
\bm{\pi}\geq\bm{0},\qquad\bm{1}^{\top}\bm{\pi}=1,\qquad\text{and}\qquad\bm{\pi}^{\top}\bm{P}=\bm{\pi}^{\top}
\end{equation*}
		\item $\bm{\pi}$ is in fact left eigenvector of $\bm{P}$ with eigenvalue $1$
		\item 1 is largest eigenvalue of $\bm{P}$ in absolute sense: $|\lambda_{i}(\bm{P})| \leq 1$ by Gershgorin circle theorem
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Reversible Markov chains}
	\begin{itemize}
		\item Markov chain $\{X_t\}_{t\geq 0}$ with transition matrix $\bm{P}$ and stationary distribution $\bm{\pi}$ is said to be \alert{reversible} if 
	\[
	\pi_{i} P_{i,j} = \pi_{j} P_{j,i} \quad \text{ for all } i, j
	\]
	{\hfill \em --- detailed balance condition}
	\item Nice consequence: if $\bm{P}$ is reversible, all eigenvalues are real \\
	{\hfill \em --- will see proof later}
	\end{itemize}
	
\end{frame}


\begin{frame}
	\frametitle{Setup}
	\begin{itemize}
		\item Probability transition matrix $\bm{P}^{\star}$ of reversible Markov chain
		\item Perturbed transition matrix $\bm{P}=\bm{P}^{\star}+\bm{E}$
		\item $\bm{\pi}^\star$, $\bm{\pi}$ are leading left eigenvectors of $\bm{P}^{\star}$, $\bm{P}$, respectively
		\item Question: how does $\bm{E}$ affect perturbation $\bm{\pi} - \bm{\pi}^{\star}$
	\end{itemize}

\end{frame}

\begin{frame}
	\frametitle{New norms}
	Fix a strictly positive probability vector $\bm{\pi}=[\pi_i]_{1\leq i\leq n}$, define 
	
	\vspace{1em}
	\begin{itemize}
		\item Vector norm: $\|\bm{x}\|_{\bm{\pi}} \coloneqq \sqrt{\sum_i \pi_i x_i^2}$ with $\bm{x}=[x_i]_{1\leq i\leq n}$
		\item Matrix norm: $\|\bm{A}\|_{\bm{\pi}} \coloneqq  \sup_{\|\bm{x}\|_{\bm{\pi}}=1}\|\bm{A}\bm{x}\|_{\bm{\pi}}$ with $\bm{A}=[A_{i,j}]_{1\leq i, j \leq n}$
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Eigenvector perturbation for transition matrices}



\begin{theorem}[Chen, Fan, Ma, Wang\,'17]
\label{thm:DK_asym}
	Suppose that $\bm{P}^{\star}$ represents a reversible Markov chain, whose stationary distribution vector $\bm{\pi}^{\star}$ is strictly positive.  Assume that
%
\begin{equation*}
	\left\Vert \bm{E}\right\Vert _{\bm{\pi}^{\star}}
	< 1-\max\big\{ \lambda_{2}(\bm{P}^{\star}),-\lambda_{n}(\bm{P}^{\star})\big\} .
\end{equation*}
%
Then one has
%
\[
	\|\bm{\pi}-\bm{\pi}^{\star}\|_{\bm{\pi}^{\star}}
	\leq
	\frac{\big\Vert \bm{\pi}^{\star\top}\bm{E}\big\Vert _{\bm{\pi}^{\star}}}{1-\max\big\{ \lambda_{2}(\bm{P}^{\star}),-\lambda_{n}(\bm{P}^{\star})\big\}
	-\left\Vert \bm{E}\right\Vert _{\bm{\pi}^{\star}}}.
\]
%
\end{theorem}

\vfill
\begin{itemize}
	\item Similar to Davis-Kahan
	\item Eigengap: $1-\max\big\{ \lambda_{2}(\bm{P}^{\star}),-\lambda_{n}(\bm{P}^{\star})\big\}$ since 1 = $\lambda_{1}(\bm{P})$
	\item Noise size: $\big\Vert \bm{\pi}^{\star\top}\bm{E}\big\Vert _{\bm{\pi}^{\star}}$
\end{itemize}
\end{frame}

\begin{frame}

	\frametitle{Proof of Theorem~\ref{thm:DK_asym}}
	By definitions of $\bm{\pi}^\star$ and $\bm{\pi}$, we have
	\[
\bm{\pi}^{\star\top}\bm{P}^{\star}=\bm{\pi}^{\star\top},\qquad\text{and}\qquad\bm{\pi}^{\top}\bm{P}=\bm{\pi}^{\top},
\]
%
which imply the following decomposition of $\bm{\pi}-\bm{\pi}^{\star}$
%
\begin{align*}
&\bm{\pi}^{\top}-\bm{\pi}^{\star\top}  =\bm{\pi}^{\top}\bm{P}-\bm{\pi}^{\star\top}\bm{P}^{\star}=\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\bm{P}+\bm{\pi}^{\star\top}\left(\bm{P}-\bm{P}^{\star}\right)\\
 &\quad =\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\left(\bm{P}-\bm{P}^{\star}\right)+\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\bm{P}^{\star}+\bm{\pi}^{\star\top}\left(\bm{P}-\bm{P}^{\star}\right)\\
 &\quad =\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\left(\bm{P}-\bm{P}^{\star}\right)
	+\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\big(\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top}\big) + \bm{\pi}^{\star\top}\left(\bm{P}-\bm{P}^{\star}\right)
\end{align*}
%
In last step, we use $\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\bm{1}= 1 - 1 = 0$
\end{frame}

\begin{frame}
	\frametitle{Proof of Theorem~\ref{thm:DK_asym} (cont.)}
	Apply triangle inequality w.r.t.~norm $\|\cdot\|_{\bm{\pi}^{\star}}$ to obtain
%
\begin{align*}
\hspace{-0.5em}\|\bm{\pi}-\bm{\pi}^{\star}\|_{\bm{\pi}^{\star}}
  & \leq \big\|\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\left(\bm{P}-\bm{P}^{\star}\right) \big\|_{\bm{\pi}^{\star}}
	+ \big\|\left(\bm{\pi}-\bm{\pi}^{\star}\right)^{\top}\big(\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top}\big) \big\|_{\bm{\pi}^{\star}} \nonumber\\
&\quad\quad + \big\|\bm{\pi}^{\star\top}\left(\bm{P}-\bm{P}^{\star}\right) \big\|_{\bm{\pi}^{\star}}\nonumber \\
 & \leq\left( \|\bm{P}-\bm{P}^{\star}\|_{\bm{\pi}^{\star}}+ \big\|\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top} \big\|_{\bm{\pi}^{\star}}\right)  \|\bm{\pi}-\bm{\pi}^{\star}\|_{\bm{\pi}^{\star}}\nonumber\\
 &\quad\quad + \big\|\bm{\pi}^{\star\top}\left(\bm{P}-\bm{P}^{\star}\right) \big\|_{\bm{\pi}^{\star}}
%\label{eq:proof-dk-asymm}
\end{align*}
%

Assuming $\|\bm{P}-\bm{P}^{\star}\|_{\bm{\pi}^{\star}}+ \|\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top}\|_{\bm{\pi}^{\star}}<1$, rearrangement gives
%
\[
	\|\bm{\pi}-\bm{\pi}^{\star}\|_{\bm{\pi}^{\star}}
	\leq\frac{\big\|\bm{\pi}^{\star\top}\left(\bm{P}-\bm{P}^{\star}\right)\big\|_{\bm{\pi}^{\star}}}{1-\|\bm{P}-\bm{P}^{\star}\|_{\bm{\pi}^{\star}}-\big\|\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top}\big\|_{\bm{\pi}^{\star}}}
\]

Proof will be complete if one can show
\begin{equation}
	\big\|\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top} \big\|_{\bm{\pi}^{\star}}
	=\max\big\{ \lambda_{2}(\bm{P}^{\star}),-\lambda_{n}(\bm{P}^{\star}) \big\} 
	\label{eq:spectral-gap}
\end{equation}

\end{frame}

\begin{frame}
	\frametitle{Proof of identity~\eqref{eq:spectral-gap}}
	Define diagonal matrix $\bm{\Pi}^{\star} = \mathsf{diag}([\pi_{1}^{\star}, \cdots, \pi_{n}^{\star}])\in\mathbb{R}^{n\times n}$. Observe
	\begin{align*}
	& \|\bm{A}\|_{\bm{\pi}^{\star}}  =\sup_{\bm{x}\neq\bm{0}}\frac{\|\bm{A}\bm{x}\|_{\bm{\pi}^{\star}}}{\|\bm{x}\|_{\bm{\pi}^{\star}}}=\sup_{\bm{x}\neq\bm{0}}\frac{\big\|\big(\bm{\Pi}^{\star}\big)^{1/2}\bm{A}\big(\bm{\Pi}^{\star}\big)^{-1/2}\big(\bm{\Pi}^{\star}\big)^{1/2}\bm{x}\big\|_{2}}{\big\|\big(\bm{\Pi}^{\star}\big)^{1/2}\bm{x}\big\|_{2}} \notag\\
	& \quad =\sup_{\bm{v}\neq\bm{0}}\frac{\big\|\big(\bm{\Pi}^{\star}\big)^{1/2}\bm{A}\big(\bm{\Pi}^{\star}\big)^{-1/2}\bm{v}\big\|_{2}}{\big\|\bm{v}\big\|_{2}}=\big\|\big(\bm{\Pi}^{\star}\big)^{1/2}\bm{A}\big(\bm{\Pi}^{\star}\big)^{-1/2}\big\|
\end{align*}
As a consequence, one has
\begin{align*}
\|\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top}\|_{\bm{\pi}^{\star}} & =\|\left(\bm{\Pi}^{\star}\right)^{1/2}\big(\bm{P}^{\star}-\bm{1}\bm{\pi}^{\star\top}\big)\left(\bm{\Pi}^{\star}\right)^{-1/2}\|\\
	& =\big\Vert \bm{S}^{\star}- \bm{\pi}^{\star}_{1/2}(\bm{\pi}^{\star}_{1/2})^{\top} \big\Vert
\end{align*}
with $\bm{S}^{\star} = \left(\bm{\Pi}^{\star}\right)^{1/2} \bm{P}^{\star} \left(\bm{\Pi}^{\star}\right)^{-1/2}$ and $\bm{\pi}_{1/2}^{\star} = [(\pi^\star_{j})^{1/2}]_{1\leq j \leq n}$
\end{frame}

\begin{frame}
	\frametitle{Proof of identity~\eqref{eq:spectral-gap} (cont.)}
	Several  properties of $\bm{S}^{\star}$:
	\begin{itemize}
		\item Symmetric: all eigenvalues are real \\
		{\hfill \em --- check detailed balance}
		\item Similar to $\bm{P}^{\star}$: $\bm{S}^{\star}$ have same eigenvalues as $\bm{P}^{\star}$, and 
		\begin{align*}
			\bm{S}^{\star} \bm{\pi}^\star_{1/2} = \bm{\pi}^\star_{1/2}
		\end{align*}
		\item Eigenvalues of $\bm{S}^{\star} -  \bm{\pi}^{\star}_{1/2}(\bm{\pi}^{\star}_{1/2})^{\top}$ are $0, \lambda_{2}(\bm{S}^{\star}), \ldots, \lambda_{n}(\bm{S}^{\star})$
	\end{itemize}
	
	\vfill
	Combine all to see
	\begin{align*}
	& \big\Vert \bm{S}^{\star} - \bm{\pi}^{\star}_{1/2}(\bm{\pi}^{\star}_{1/2})^{\top} \big\Vert
	\overset{(\mathrm{i})}{=} \max \big\{ \big| \lambda_{2}(\bm{S}^{\star}) \big|, \big| \lambda_{n}(\bm{S}^{\star}) \big| \big\}  \\
	& \qquad = \max \big\{ \lambda_{2}(\bm{S}^{\star}),-\lambda_{n}(\bm{S}^{\star}) \big\}
	 \overset{\mathrm{(ii)}}{=}\max\big\{ \lambda_{2}(\bm{P}^{\star}),-\lambda_{n}(\bm{P}^{\star}) \big\} .
\end{align*}
\end{frame}

%
%\begin{frame}[allowframebreaks]
%\frametitle{Reference}
%
%{\small
%\begin{itemize}
%		%[{\color{black}[1]}] 
%
%  \itemsep0.3em
%  \item ''\textit{The rotation of eigenvectors by a perturbation},'' C.~Davis, W.~Kahan, \textit{SIAM Journal on Numerical Analysis}, 1970.
%
%  \item ''\textit{Perturbation bounds in connection with singular value decomposition},'' P.~Wedin, \textit{BIT Numerical Mathematics}, 1972.
%
%
%  \item ''\textit{COMS 4772 lecture notes},'' D.~Hsu, Columbia University.
%
%
%  %\item ''\textit{Principal component analysis for big data},'' J.~Fan, Q.~Sun, W.~Zhou, Z.~Zhu,  \textit{arXiv:1801.01602}, 2018. 
%
%  \item ''\textit{Community detection and stochastic block models},'' E.~Abbe, \textit{Foundations and Trends in Communications and Information Theory}, 2018.
%
%  \item ''\textit{Consistency thresholds for the planted bisection model},'' E.~Mossel, J.~Neeman, A.~Sly, \textit{ACM Symposium on Theory of Computing}, 2015.
%
%  \item ''\textit{Matrix completion from a few entries},'' R.~Keshavan, A.~Montanari, S.~Oh, \textit{IEEE Transactions on Information Theory}, 2010.
%
%
%  \item ''\textit{The PageRank citation ranking: bringing order to the web},'' L.~Page, S.~Brin, R.~Motwani, T.~Winograd, 1999. 
%
%  \item ''\textit{Rank centrality: ranking from pairwise comparisons},'' S.~Negahban, S.~Oh, D.~Shah, \textit{Operations Research}, 2016. 
%
%  \item ''\textit{Spectral method and regularized MLE are both optimal for top-$K$ ranking},'' Y.~Chen, J.~Fan, C.~Ma, K.~Wang, \textit{Annals of Statistics}, 2019.
%
%\end{itemize}
%}
%
%
%\end{frame}



\end{document}

