<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<p>STAT 253/317
Home            <a href="index.html" target=&ldquo;blank&rdquo;>index.html</a> 
Course Info     <a href="course_info.html" target=&ldquo;blank&rdquo;>course_info.html</a>
Syllabus	    <a href="syllabus.html" target=&ldquo;blank&rdquo;>syllabus.html</a>
Lectures	    <a href="lectures.html" target=&ldquo;blank&rdquo;>lectures.html</a>


Reference       <a href="reference.html" target=&ldquo;blank&rdquo;>reference.html</a>
</p>
<ol>
<li><p>&lsquo;&lsquo;The landscape of empirical risk for non-convex losses,&rsquo;&rsquo; S. Mei, Y. Bai, and A. Montanari, <i>The Annals of Statistics</i>, 2018.



</p>
</li>
<li><p>&lsquo;&lsquo;Gradient Descent Learns Linear Dynamical Systems, &rsquo;&rsquo; M. Hardt, T. Ma, B. Recht, <i>Journal of Machine Learning Research</i>, 2018


</p>
</li>
<li><p>&lsquo;&lsquo;Universality laws for randomized dimension reduction, with applications,&rsquo;&rsquo; S. Oymak, and Joel A. Tropp, <i>Information and Inference</i>, 2017.
</p>
</li>
<li><p>&lsquo;&lsquo;Phase transitions in semidefinite relaxations,&rsquo;&rsquo; A. Javanmard, A. Montanari, and F. Ricci-Tersenghi,  <i>Proceedings of the National Academy of Sciences</i>, 2016.

</p>
</li>
<li><p>&lsquo;&lsquo;On the Optimization Landscape of Tensor Decompositions,&rsquo;&rsquo; R. Ge and T. Ma, <i>Advances in Neural Information Processing Systems</i>, 2017.
</p>
</li>
<li><p>&lsquo;&lsquo;SLOPE is adaptive to unknown sparsity and asymptotically minimax,&rsquo;&rsquo; W. Su and E. Candes, <i>The Annals of Statistics</i>, 2016.
</p>
</li>
<li><p>&lsquo;&lsquo;Spectral methods meet EM: A provably optimal algorithm for crowdsourcing,&rsquo;&rsquo; Y. Zhang, X. Chen, D. Zhou, and M. Jordan,  <i>Advances in Neural Information Processing Systems</i>, 2014.



</p>
</li>
<li><p>&lsquo;&lsquo;Tensor SVD: Statistical and Computational Limits,&rsquo;&rsquo; A. Zhang, D. Xia, <i>IEEE Transactions on Information Theory</i>, 2018. 

</p>
</li>
<li><p>&lsquo;&lsquo;No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis,&rsquo;&rsquo; R. Ge, C. Jin, Y. Zheng, <i>International Conference on Machine Learning</i>, 2017. 
</p>
</li>
<li><p>&lsquo;&lsquo;Is Q-learning Provably Efficient?&rsquo;&rsquo; C. Jin, Z. Allen-Zhu, S. Bubeck, M. Jordan, <i>Advances in Neural Information Processing Systems</i>, 2018. 
</p>
</li>
<li><p>&lsquo;&lsquo;Nonconvex Low-Rank Symmetric Tensor Completion from Noisy Data,&rsquo;&rsquo; C. Cai, G. Li, H. V. Poor, Y. Chen, <i>Advances in Neural Information Processing Systems</i>, 2019.
</p>
</li>
<li><p>&lsquo;&lsquo;The Landscape of the Spiked Tensor Model,&rsquo;&rsquo; G. Arous, S. Mei, A. Montanari, and M. Nica, <i>Communications on Pure and Applied Mathematics</i>, 2019.
</p>
</li>
<li><p>&lsquo;&lsquo;Learning Mixtures of Low-Rank Models,&rsquo;&rsquo; Y. Chen, C. Ma, H. V. Poor, Y. Chen, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Self-regularizing Property of Nonparametric Maximum Likelihood Estimator in Mixture Models,&rsquo;&rsquo; Y. Polyanskiy, Y. Wu, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations,&rsquo;&rsquo; Y. Li, T. Ma, H. Zhang, COLT 2018. 
</p>
</li>
<li><p>&lsquo;&lsquo;Inference and Uncertainty Quantification for Noisy Matrix Completion,&rsquo;&rsquo; Y. Chen, J. Fan, C. Ma, and Y. Yan, <i>Proceedings of the National Academy of Sciences (PNAS)</i>, 2019. 
</p>
</li>
<li><p>&lsquo;&lsquo;The Lasso with General Gaussian Designs with Applications to Hypothesis Testing,&rsquo;&rsquo; M. Celentano,  A. Montanari, Y. Wei, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Matrix concentration for products,&rsquo;&rsquo;  D. Huang, J. Niles-Weed, J. Tropp, and R. Ward, 2020.
</p>
</li>
<li><p>&lsquo;&lsquo;Meta-learning for Mixed Linear Regression,&rsquo;&rsquo; W. Kong, R. Somani, Z. Song, S. Kakade, S. Oh, <i>International Conference on Machine Learning</i>, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Nonconvex Matrix Completion with Linearly Parameterized Factors,&rsquo;&rsquo; J. Chen, X. Li, and Z. Ma, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Low-rank Matrix Recovery with Composite Optimization: Good Conditioning and Rapid Convergence,&rsquo;&rsquo; V. Charisopoulos, Y. Chen, D. Davis, M. Diaz, L. Ding, D. Drusvyatskiy, 2019. 
</p>
</li>
<li><p>&lsquo;&lsquo;Toward the Fundamental Limits of Imitation Learning,&rsquo;&rsquo; N. Rajaraman, L. Yang, J. Jiao, K. Ramachandran, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Robust Estimation via Generalized Quasi-gradients,&rsquo;&rsquo; B. Zhu, J. Jiao, J. Steinhardt, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Decoupling Representation Learning from Reinforcement Learning,&rsquo;&rsquo; A. Stooke, K. Lee, P. Abbeel, M. Laskin, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?&rsquo;&rsquo;  S. Du, S. Kakade, R. Wang, L. Yang, <i>International Conference on Learning Representations</i>, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Deep Networks and the Multiple Manifold Problem,&rsquo;&rsquo; S. Buchanan, D. Gilboa, J. Wright, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model,&rsquo;&rsquo; G. Li, Y. Wei, Y. Chi, Y. Gu, Y. Chen, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Two Models of Double Descent for Weak Features,&rsquo;&rsquo; M. Belkin, D. Hsu, J. Xu, 2019. 
</p>
</li>
<li><p>&lsquo;&lsquo;Kernel and Rich Regimes in Overparametrized Models,&rsquo;&rsquo; B. Woodworth, S. Gunasekar, J. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, N. Srebro, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Just Interpolate: Kernel Ridgeless Regression Can Generalize,&rsquo;&rsquo; T. Liang, A. Rakhlin, <i>The Annals of Statistics</i>, 2020.
</p>
</li>
<li><p>&lsquo;&lsquo;Benign Overfitting in Linear Regression,&rsquo;&rsquo; P. Bartlett, P. Long,  G. Lugosi, and A. Tsigler, <i>Proceedings of the National Academy of Sciences (PNAS)</i>, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Consistent Risk Estimation in High-Dimensional Linear Regression,&rsquo;&rsquo; J. Xu, A. Maleki, K. Rad, 2019. 
</p>
</li>
<li><p>&lsquo;&lsquo;Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification,&rsquo;&rsquo; C. Dan, Y. Wei, P. Ravikumar, <i>International Conference on Machine Learning</i>, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Learning Models with Uniform Performance via Distributionally Robust Optimization,&rsquo;&rsquo; J. Duchi and H. Namkoong, <i>The Annals of Statistics</i>, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;The Importance of Better Models in Stochastic Optimization,&rsquo;&rsquo; H. Asi and J. Duchi, <i>Proceedings of the National Academy of Sciences (PNAS)</i>, 2019.  
</p>
</li>
<li><p>&lsquo;&lsquo;Gaussian Differential Privacy,&rsquo;&rsquo; J. Dong, A. Roth, W. Su, <i>Journal of the Royal Statistical Society: Series B</i>, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Precise Tradeoffs in Adversarial Training for Linear Regression,&rsquo;&rsquo; A. Javanmard, M. Soltanolkotabi, H. Hassani, 2020. 
</p>
</li>
<li><p>&lsquo;&lsquo;Robust Estimation via Robust Gradient Estimation,&rsquo;&rsquo; A. Prasad, A. Suggala, S. Balakrishnan,  P. Ravikumar,  <i>Journal of the Royal Statistical Society, Series B</i>, 2020.
</p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
Page generated 2026-01-07 22:32:44 CST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</div>
</body>
</html>
