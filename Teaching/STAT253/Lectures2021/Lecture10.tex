%\documentclass[letterpaper,draft]{beamer}
\documentclass[letterpaper,handout, mathserif]{beamer}
%\documentclass[letterpaper]{beamer}

%---multiple pages on one sheet, ADD for handout--
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=1mm]
%-------------------------------------------------
\usepackage{amsmath,amsfonts}
%\usepackage[misc]{ifsym} % for the dice symbol \Cube{}
%\usepackage{booktabs}
%\usepackage{mdwlist}
%\usepackage{pgf,tikz}
%\usetheme{Copenhagen}
%\usetheme{warsaw}
\setbeamertemplate{navigation symbols}{}
\usepackage[english]{babel}
\def\ul{\underline}
% or whatever

\usepackage[latin1]{inputenc}
\subject{Talks}

\def\Sum{\sum\nolimits}
\def\Prod{\prod\nolimits}
\def\p{\mathrm P}
\def\E{\mathbb E}
\def\V{\mathrm Var}
\def\typo#1{\alert{#1}}
%-------------Answers------------
\def\Hide#1#2{\ul{~~~\onslide<#1>{\alert{#2}}~~~}}
\def\hide#1#2{\ul{~~\onslide<#1>{\alert{#2}}~~}}
%------Centered Page Number------
\input{Centerpgn}
\def\chapnum{10}
%--------------------------------
\setbeamertemplate{footline}[centered page number]

\title{STAT253/317 Lecture \chapnum} \date{} \author{Cong Ma}
\begin{document}
% ----------------------------------------------------------------------
\begin{frame}\maketitle\begin{center}5.3 \ The Poisson Processes\end{center}\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Properties of Poisson Processes}
Outline:
\begin{itemize}
\item Interarrival times of events are i.i.d Exponential with rate $\lambda$
\item Conditional Distribution of the Arrival Times
\item Superposition \& Thinning \dotfill (Lecture 11)
\item ``Converse'' of Superposition \& Thinning \dotfill (Lecture 11)
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Arrival \& Interarrival Times of Poisson Processes}
Let
\begin{align*}
S_n &=\text{Arrival time of the } n\text{-th event},n=1,2,\ldots\\
T_1%&=\inf\{t>0:  N(t)=1\}\\
   &=S_1=\text{Time until the 1st event occurs}\\
T_n%&=\inf\{t>T_{n-1}: N(t)=n\}-T_{n-1}\\
   &=S_n-S_{n-1}\\
   &=\text{time elapsed between the $(n-1)$st and $n$-th event},\\
   &\quad n=2,3,\ldots
\end{align*}
\begin{block}{Proposition 5.1}
The interarrival times $T_1,T_2,\ldots,T_k,\ldots,$ are i.i.d $\sim Exp(\lambda)$.\bigskip
\end{block}
Consequently, as the distribution of the sum of $n$ i.i.d Exp$(\lambda)$ is $Gamma(n,\lambda)$,
the arrival time of the $n$th event is
 $$S_n =\sum_{i=1}^n T_i\sim Gamma(n,\lambda)$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Proof of Proposition 5.1}
\begin{align*}
&\p(T_{n+1} > t |T_1 = t_1, T_2=t_2,\ldots,T_n=t_n)\\
={}& \p(0\text{ event in }(s_n, s_n + t] |T_1 = t_1, T_2=t_2,\ldots,T_n=t_n)\\
&\hspace{1.5in}(\text{where }s_n=t_1+t_2+\cdots+t_n)\\
={}& \p(0 \text{ event in }(s_n, s_n + t ]) \quad (\text{by indep increment})\\
={}& \p(N(s_n + t)-N(s_n)=0)\\
={}& e^{-\lambda t}
\end{align*}
where the last step comes from the fact that
\begin{itemize}
\item $N(s_n + t)-N(s_n)\sim$ Poisson$(\lambda t)$ and
\item $P(N = k) = e^{-\mu}\mu^k/k!$ if $N\sim$ Poisson$(\mu)$, $k=0,1,2\ldots$
\end{itemize}
This shows that $T_{n+1}$ is $\sim Exp(\lambda)$, and is independent of $T_1,T_2,\ldots,T_n$.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Definition 3 of the Poisson Process}
A continuous-time stochastic process $\{N(t), t\ge 0\}$ is a Poisson process with rate $\lambda>0$  if
\begin{itemize}
\item [(i)] $N(0)=0$,
\item [(ii)] $N(t)$ counts the number of events that have occurred up to time
t (i.e., it is a counting process).
\item [(iii)] The times between events are independent and identically distributed
with an Exp$(\lambda)$ distribution.
\end{itemize}\bigskip
We have seen how Definition 5.1 implies (i), (ii) and (iii) in Definition 3.
The proof of the converse is omitted.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{5.3.5 Conditional Distribution of Arrival Times is Uniform}

Given $N(t)=1$, then $T_1$, the arrival time of the first event $\sim \text{Uniform}(0,t)$

{\em Proof.} For $s<t$,
\begin{align*}\small
&\p(T_1\le s|N(t)\!=\!1)=\frac{\p(T_1\le s, N(t)=1)}{\p(N(t)=1)}\\
&=\frac{\p(\text{1 event in $(0,s]$, no events in $(s,t]$})}{\p(N(t)=1)}\\
%&=\frac{\p(N(s)=1, N(t)-N(s)=0)}{\p(N(t)=1)}\\
&=\frac{\p(N(s)\!=\!1)\p(N(t)\!-\!N(s)\!=\!0)}{\p(N(t)=1)}\text{ by indep. increment}\\
&=^*\frac{(\lambda s e^{-\lambda s})(e^{-\lambda (t-s)})}{\lambda t e^{-\lambda t}}=\frac{s}{t},\quad s< t.
\end{align*}\normalsize
where the step $=^*$ comes from the fact that
\begin{itemize}
\item $N(s)\sim$ Poisson$(\lambda s)$, $N(t)-N(s)\sim$ Poisson$(\lambda (t-s))$, and $N(t)\sim$ Poisson$(\lambda t)$
\item $P(N = k) = e^{-\mu}\mu^k/k!$ if $N\sim$ Poisson$(\mu)$, $k=0,1,2\ldots$
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Review of Order Statistics}
Suppose $X_1$, $X_2$,\ldots, $X_n$ are i.i.d. random variables with a common density $f(x)$.
Their joint density would be the product of the marginal density
\[
f(x_1,x_2,\ldots,x_n)=f(x_1)f(x_2)\ldots f(x_n).
\]
Let $X_{(i)}$ be the $i$th smallest number among $X_1,X_2,\ldots, X_n$.
$(X_{(1)}, X_{(2)}, \ldots, X_{(n)})$ is called the order statistics of $X_1,X_2,\ldots, X_n$
\begin{itemize}
\item $X_{(1)}$ is the minimum
\item $X_{(n)}$ is the maximum
\item $X_{(1)}\le X_{(2)}\le \ldots\le X_{(n)}$
\end{itemize}
The joint density of $X_{(1)}, X_{(2)}, \ldots, X_{(n)}$ is
\[
h(x_1,x_2,\ldots,x_n)=
\begin{cases}
n!f(x_1)f(x_2)\ldots f(x_n), &\text{if }x_1\le x_2\le\ldots\le x_n.\\
0 &\text{otherwise}
\end{cases}
\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example}
If $U_1, U_2, \ldots, U_n$ are indep. Uniform$(0,t)$,
their common density is
$$
f(u)=
\begin{cases}
1/t,& \text{for }0<u<t.\\
0 &\text{otherwise}
\end{cases}
$$
The joint density of their order statistics $U_{(1)}, U_{(2)},\ldots, U_{(n)}$ is
$$
h(u_1,u_2,\ldots,u_n)=n! f(u_1)f(u_2)\ldots f(u_n)=n!(1/t)^n
$$
for $0\le u_1\le u_2\le \ldots\le u_n< t$ and 0 elsewhere.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Theorem 5.2}
Given $N(t)=n$, $$(S_1,S_2,\ldots,S_n)\sim(U_{(1)},U_{(2)},\ldots,U_{(n)})$$
where $(U_{(1)},\ldots,U_{(n)})$ are the order statistics of $(U_1,\ldots,U_n)\sim$ i.i.d Uniform$\,(0,t)$, i.e., the joint conditional density of $S_1,S_2,\ldots$, $S_n$ is
$$
f(s_1,s_2,\ldots,s_n|N(t)=n)=n!/t^n,\; 0<s_1<s_2<\ldots<s_n
$$
{\em Proof.}
The event that $S_1 = s_1$, $S_2 = s_2, \ldots$, $S_n = s_n$, $N(t) = n$ is equivalent to the event $T_1 = s_1$, $T_2 = s_2- s_1,\ldots, T_n = s_n- s_{n-1}$, $T_{n+1} > t-s_n$. Hence, by
Proposition 5.1, we have the conditional joint density of $S_1,\ldots, S_n$ given $N(t) = n$ as follows:
\begin{align*}
&f(s_1,\ldots, s_n| N(t) = n)= \frac{f(s_1,\ldots, s_n,N(t) = n)}{\p(N(t)=n)}\\
&=\frac{\lambda e^{-\lambda s_1}\lambda e^{-\lambda (s_2-s_1)}\ldots\lambda e^{-\lambda (s_n-s_{n-1})}
e^{-\lambda (t-s_n)}}{e^{-\lambda t} (\lambda t)^n/n!}\\
&=n!t^{-n},\quad 0 < s_1 <\ldots< s_n < t
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}\noindent{\bf Example 5.21}.
Insurance claims comes according to a Poisson process $\{N(t)\}$ with rate $\lambda$. Let
\begin{itemize}
\item $S_i=$ the time of the $i$th claims
\item $C_i=$ amount of the $i$th claims, i.i.d with mean $\mu$, indep. of $\{N(t)\}$
\end{itemize}
Then the  total discounted cost by time $t$ at discount rate $\alpha$ is given by
$$D(t) =\Sum_{i=1}^{N(t)}C_i e^{-\alpha S_i}.$$
Then
\begin{align*}
\E[D(t)|N(t)]&=\E\left[\Sum_{i=1}^{N(t)}C_i e^{-\alpha S_i}\Big|N(t)\right]\overset{(5.2)}{=}\E\left[\Sum_{i=1}^{N(t)}C_i e^{-\alpha U_{(i)}}\right]\\
&=\E\left[\Sum_{i=1}^{N(t)}C_i e^{-\alpha U_i}\right]=\Sum_{i=1}^{N(t)}\E[C_i]\E\left[e^{-\alpha U_i}\right]\\
&=N(t)\mu\int_0^t\frac{1}{t}e^{-\alpha x}dx=N(t)\frac{\mu}{\alpha t}(1-e^{-\alpha t})
\end{align*}
Thus $\E[D(t)]=\E[N(t)]\frac{\mu}{\alpha t}(1-e^{\alpha t})=\frac{\lambda\mu}{\alpha }(1-e^{-\alpha t})$
\end{frame}
% ----------------------------------------------------------------------
\end{document}
\begin{frame}{Proposition 5.3 (Generalization of Proposition 5.2)}
Consider a Poisson process with rate $\lambda$.
For an event occurs at time $t$,
it is classified as a type $i$ event with probability $p_i$, $i=1,2,\ldots,k$,
 $\sum_i p_i =1$, independently of all other events.
Let $$N_i(t)= \text{number of type }i \text{ events occurring in }[0, t], \ i = 1,2,\ldots,k$$
Note $N(t)=\sum_{i=1}^{k}N_i(t)$.

$\{N_1(t), t \ge0\},\ldots,\{N_k(t), t\ge 0\}$ are all Poisson processes
having respective rates $\lambda p_i$, $i=1,\ldots,k$. Furthermore, the $k$ processes are independent

\end{frame}
% ----------------------------------------------------------------------
