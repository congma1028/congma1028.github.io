%\documentclass[letterpaper,draft]{beamer}
%\documentclass[letterpaper,handout]{beamer}
\documentclass[letterpaper]{beamer}

%---multiple pages on one sheet, ADD for handout--
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=1mm]
%-------------------------------------------------
\usepackage{amsmath,amsfonts}
%\usepackage[misc]{ifsym} % for the dice symbol \Cube{}
%\usepackage{booktabs}
%\usepackage{mdwlist}
%\usepackage{pgf,tikz}
%\usetheme{Copenhagen}
%\usetheme{warsaw}
\setbeamertemplate{navigation symbols}{}
\usepackage[english]{babel}
\def\ul{\underline}
% or whatever

\usepackage[latin1]{inputenc}
\subject{Talks}

\def\Sum{\sum\nolimits}
\def\p{\mathrm P}
\def\E{\mathbb E}
\def\V{\mathrm{Var}}
%-------------Answers------------
\def\Hide#1#2{\ul{~~~\onslide<#1>{\alert{#2}}~~~}}
\def\hide#1#2{\ul{~~\onslide<#1>{\alert{#2}}~~}}
%------Centered Page Number------
\input{Centerpgn}
\def\chapnum{7}
%--------------------------------
\setbeamertemplate{footline}[centered page number]

\title{STAT253/317 Lecture \chapnum}
\date{}
\author{Yibi Huang}
\begin{document}
% ----------------------------------------------------------------------
\begin{frame}\maketitle
\begin{center}
\begin{tabular}{ll}
$\bullet$ & Using the Recursive Relations of Markov Chains\\
4.5.3 & Random Walk w/ Reflective Boundary at 0\\
4.7   & Branching Processes
\end{tabular}
\end{center}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Using the Recursive Relations of Markov Chains}
Consecutive terms in many Markov chains $\{X_n\}$ often have some recursive relations like
$$
X_{n+1}= g(X_n, \xi_{n+1})\quad\text{for all }n
$$
where $\{\xi_n, n=0,1,2,\ldots\}$ are some i.i.d. random variables and $X_n$ is independent of $\{\xi_k: k>n\}$.
\par\medskip

In many cases, we can use the recursive relationship to find $\E[X_n]$ and $\V[X_n]$ without knowing the distribution of $X_n$.
\begin{align*}
\E[X_{n+1}]&=\E[\E[X_{n+1}|X_{n}]]\\
\V(X_{n+1})&= \E[\V(X_{n+1}|X_{n})] + \V(\E[X_{n+1}|X_{n}])
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 1: Simple Random Walk}
$$
X_{n+1}=
\begin{cases}
X_n + 1 & \text{with prob }p\\
X_n - 1 & \text{with prob }q=1-p
\end{cases}
$$
So
\begin{align*}
\E[X_{n+1}|X_n] &= p (X_n+1) + q(X_n-1) = X_n + p-q\\
\V[X_{n+1}|X_n] &= 4pq
\end{align*}
Then
\begin{align*}
\E[X_{n+1}]&=\E[\E[X_{n+1}|X_n]] = \E[X_n] + p-q\\
\V(X_{n+1})&= \E[\V(X_{n+1}|X_{n})] + \V(\E[X_{n+1}|X_{n}])\\
&=\E[4pq]+\V(X_n + p-q) = 4pq + \V(X_n)
\end{align*}
So
$$
\E[X_n]=n(p-q)+\E[X_0],\qquad \V(X_n) = 4npq + \V(X_0)
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 2: Ehrenfest Urn Model with $M$ Balls}
Recall that
$$
X_{n+1}=
\begin{cases}
X_n+1 & \text{with probability } \frac{M-X_n}{M}\\
X_n-1 & \text{with probability } \frac{X_n}{M}
\end{cases}
$$
We have
$$
\E[X_{n+1}|X_n]=(X_n+1)\times \frac{M-X_n}{M}+(X_n-1)\times\frac{X_n}{M}=1+\left(1-\frac{2}{M}\right)X_n.
$$
Thus
$$
\E[X_{n+1}]=\E[\E[X_{n+1}|X_n]]=1+\left(1-\frac{2}{M}\right)\E[X_n]
$$
Subtracting $M/2$ from both sides of the equation above, we get
$$
\E[X_{n+1}]-\frac{M}{2}=\left(1-\frac{2}{M}\right)(\E[X_n]-\frac{M}{2})
$$
Thus
\begin{align*}
\E[X_n]-\frac{M}{2}
%&=\left(1-\frac{2}{M}\right)(\E[X_{n-1}]-\frac{M}{2})\\
%&=\left(1-\frac{2}{M}\right)\left(1-\frac{2}{M}\right)(\E[X_{n-2}]-\frac{M}{2})=\left(1-\frac{2}{M}\right)^2(\E[X_{n-2}]-\frac{M}{2})\\
%&=\left(1-\frac{2}{M}\right)^2\left(1-\frac{2}{M}\right)(\E[X_{n-3}]-\frac{M}{2})=\left(1-\frac{2}{M}\right)^3\E[X_{n-3}]-\frac{M}{2})\\
%&=\vdots\\
&=\left(1-\frac{2}{M}\right)^n(\E[X_{0}]-\frac{M}{2})
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Variance of Ehrenfest Urn Model}
$$
\E[X_{n+1}|X_n] = 1+\left(1-\frac{2}{M}\right)X_n,\quad \V(X_{n+1}|X_n) = \frac{4X_n(M-X_n)}{M^2}
$$
and hence
\begin{align*}
\V(\E[X_{n+1}|X_n]) &= \V(1+\left(1-\frac{2}{M}\right)X_n)=\left(1-\frac{2}{M}\right)^2\V(X_n)\\
\E[\V(X_{n+1}|X_n)] &= \frac{4\E[X_n(M-X_n)]}{M^2}=\frac{4}{M}\E[X_n]-\frac{4}{M^2}\E[X_n^2]\\
&=\frac{4}{M}\E[X_n]-\frac{4}{M^2}\left(\V(X_n)+(\E[X_n])^2\right)\\
&=-\frac{4}{M^2}\V(X_n)+4\frac{\E[X_n]}{M}\left(1-\frac{\E[X_n]}{M}\right)
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
So
\begin{align*}
\V(X_{n+1})&=\V(\E[X_{n+1}|X_n])+\E[\V(X_{n+1}|X_n)]\\
&=\left(1\!-\!\frac{2}{M}\right)^2\V(X_n)-\frac{4}{M^2}\V(X_n)+4\frac{\E[X_n]}{M}\left(1-\frac{\E[X_n]}{M}\right)\\
&=\left(1-\frac{4}{M}\right)\V(X_n)+4\frac{\E[X_n]}{M}\left(1-\frac{\E[X_n]}{M}\right)
\end{align*}
Recall $\E[X_n]=\frac{M}{2}+\left(1-\frac{2}{M}\right)^n(\E[X_{0}]-\frac{M}{2})$. So
\begin{align*}
\frac{\E[X_n]}{M}&=\frac{1}{2}+\left(1-\frac{2}{M}\right)^n\left(\frac{\E[X_{0}]}{M}-\frac{1}{2}\right),\\
1-\frac{\E[X_n]}{M}&=\frac{1}{2}-\left(1-\frac{2}{M}\right)^n\left(\frac{\E[X_{0}]}{M}-\frac{1}{2}\right)
\end{align*}
and their product is
$$
\frac{\E[X_n]}{M}\left(1-\frac{\E[X_n]}{M}\right)
=\frac{1}{4}-\left(1-\frac{2}{M}\right)^{2n}\left(\frac{\E[X_{0}]}{M}-\frac{1}{2}\right)^2
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}\small
$$
\V(X_{n+1})=\left(1-\frac{4}{M}\right)\V(X_n)+1-\left(1-\frac{2}{M}\right)^{2n}\left(\frac{2\E[X_{0}]}{M}-1\right)^2
$$
Subtracting $M/4$ from both sides, we get
$$
\V(X_{n+1})-\frac{M}{4}=\left(1-\frac{4}{M}\right)\left(\V(X_n)-\frac{M}{4}\right)-\left(1-\frac{2}{M}\right)^{2n}\left(\frac{2\E[X_{0}]}{M}-1\right)^2
$$

$$
v_{n+1}=av_n-cb^n
$$
\begin{align*}
\sum_{n=0}^{\infty}v_{n+1}s^{n+1}&=\sum_{n=0}^{\infty}av_ns^{n+1}-c\sum_{n=0}^{\infty}b^ns^{n+1}\\
g(s)-v_0 &=asg(s)-\frac{cs}{1-bs}\\
(1-as)g(s)&=v_0-\frac{cs}{1-bs}\\
g(s)&=\frac{v_0}{1-as}-\frac{cs}{(1-bs)(1-as)}\\
&=\frac{v_0}{1-as}-\frac{c}{b-a}\left(\frac{1}{1-bs}-\frac{1}{1-as}\right)\\
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
$b=(1-\frac{2}{M})^2$, $a=1-4/M$, $b-a=4/M^2$.
\begin{align*}
g(s)&=\frac{v_0}{1-as}-\frac{c}{b-a}\left(\frac{1}{1-bs}-\frac{1}{1-as}\right)\\
&=\left(v_0+\frac{c}{b-a}\right)\frac{1}{1-as}-\frac{c}{b-a}\frac{1}{1-bs}\\
&=\left(v_0+\frac{c}{b-a}\right)\sum_{n=0}^{\infty}a^ns^n-\frac{c}{b-a}\sum_{n=0}^{\infty}b^ns^n
\end{align*}
$\frac{c}{b-a}=(M^2/4)\left(\frac{2\E[X_{0}]}{M}-1\right)^2=\left(\E[X_{0}]-\frac{M}{2}\right)^2$
So
\begin{align*}
\V(X_n)-\frac{M}{4}&=\left(\V(X_0)-\frac{M}{4}+\left(\E[X_{0}]-\frac{M}{2}\right)^2\right)\left(1-\frac{4}{M}\right)^n\\
&\quad-\underbrace{\left(\E[X_{0}]-\frac{M}{2}\right)^2\left(1-\frac{2}{M}\right)^{2n}}_{=(\E[X_n]-M/2)^2}
\end{align*}
%\begin{align*}
%\V(X_n)-\frac{M}{4}+(\E[X_n]-\frac{M}{2})^2=\left(\V(X_0)-\frac{M}{4}+\left(\E[X_{0}]-\frac{M}{2}\right)^2\right)\left(1-\frac{4}{M}\right)^n
%\end{align*}

\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
\begin{align*}
&\V(X_n)-\frac{M}{4}+(\E[X_n]-\frac{M}{2})^2\\
=\,&\E(X_n^2)-(\E[X_n])^2-\frac{M}{4}+(\E[X_n])^2-M\E[X_n]+\frac{M^2}{4}\\
=\,&\E(X_n^2)-\frac{M}{4}-M\E[X_n]+\frac{M^2}{4}\\
=\,&\E(X_n(X_n-M))+\frac{M(M-1)}{4}
\end{align*}
$$
X_{n+1}=
\begin{cases}
X_n+1 & \text{with probability } \frac{M-X_n}{M}\\
X_n-1 & \text{with probability } \frac{X_n}{M}
\end{cases}
$$

\begin{align*}
X_{n+1}(X_{n+1}-M)&=
\begin{cases}
(X_n+1)(X_n-M+1) & \text{w. p. } \frac{M-X_n}{M}\\
(X_n-1)(X_n-M-1) & \text{w. p. } \frac{X_n}{M}
\end{cases}\\
&=
\begin{cases}
X_n(X_n-M)+1-M+2X_n & \text{w. p. } \frac{M-X_n}{M}\\
X_n(X_n-M)+1+M-2X_n & \text{w. p. } \frac{X_n}{M}
\end{cases}\\
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}

\begin{align*}
&\E(X_{n+1}(X_{n+1}-M)|X_n)\\
=\,&X_n(X_n-M)+1-(2X_n-M)^2/M\\
=\,&X_n(X_n-M)+1-(4X_n^2-4MX_n+M^2)/M\\
=\,&X_n(X_n-M)+1-4X_n^2/M-4X_n+M\\
=\,&X_n(X_n-M)+1-4X_n(X_n-M)/M+M\\
=\,&X_n(X_n-M)(1-4/M)+1+M\\
\end{align*}


\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
\begin{align*}
\V(X_1)-\frac{M}{4}&=\left(\V(X_0)-\frac{M}{4}+\left(\E[X_{0}]-\frac{M}{2}\right)^2\right)\left(1-\frac{4}{M}\right)\\
&\quad-\left(\E[X_{0}]-\frac{M}{2}\right)^2\left(1-\frac{2}{M}\right)^{2}\\
&=\left(\V(X_0)-\frac{M}{4}\right)\left(1-\frac{4}{M}\right)\\
&\quad+\left(\E[X_{0}]-\frac{M}{2}\right)^2\underbrace{\left(1-\frac{4}{M}-\left(1-\frac{2}{M}\right)^{2}\right)}_{=-4/M^2}\\
&=\left(\V(X_0)-\frac{M}{4}\right)\left(1-\frac{4}{M}\right)-\left(\frac{2\E[X_{0}]}{M}-1\right)^2
\end{align*}

\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 3: Branching Processes (Section 4.7)}
Consider a population of individuals.
\begin{itemize}
\item All individuals have the same lifetime
\item Each individual will produce a random number of offsprings at the end of its life
\end{itemize}
Let $X_n=$ size of the $n$-th generation, $n=0,1,2,\ldots$.

If $X_{n-1}=k$, the $k$ individuals in the $(n-1)$-th generation will independently produce $Z_{n,1}$, $Z_{n,2}$, \ldots, $Z_{n,k}$ new offsprings, and $Z_{n,1}$, $Z_{n,2}$, \ldots, $Z_{n,X_{n-1}}$ are i.i.d such that
$$P(Z_{n,i}=j)=P_j,\; j \ge 0.$$
We suppose that $P_j < 1$ for all $j \ge 0$.
\begin{equation}\label{eq:branching}
X_{n} =\Sum_{i=1}^{X_{n-1}}Z_{n,i}
\end{equation}
$\{X_n\}$ is a Markov chain with state space $=\{0,1,2,\ldots\}$ .
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Mean of a Branching Process}
Let $\mu=\E[Z_{n,i}]=\Sum_{j=0}^{\infty}jP_j$ be the mean \# of offsprings produced by an individual.
Since $X_{n} =\Sum_{i=1}^{X_{n-1}}Z_{n,i}$ and $Z_{n,i}$'s are i.i.d., we have
$$
\E[X_{n}|X_{n-1}]=\E\left[\Sum_{i=1}^{X_{n-1}}Z_{n,i}\Big|X_{n-1}\right]=X_{n-1}\E[Z_{n,i}]=X_{n-1}\mu
$$
So
$$
\E[X_{n}]=\E[\E[X_{n}|X_{n-1}]]=\E[X_{n-1}\mu]=\mu\E[X_{n-1}]
$$
Then
$$
\E[X_{n}]=\mu\E[X_{n-1}]=\mu^2\E[X_{n-2}]=\ldots=\mu^n\E[X_{0}]
$$

\vspace{-6pt}
\begin{itemize}
\item If $\mu<1 \Rightarrow \E[X_{n}]\to 0$ as $n\to\infty \Rightarrow \lim_{n\to\infty}\p(X_n\ge1)=0$
the branching processes will eventually die out.
\item What if $\mu=1$ or $\mu>1$?
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Variance of a Branching Process}
Let $\sigma^2=\V[Z_{n,i}]=\Sum_{j=0}^{\infty}(j-\mu)^2P_j$.
$\V(X_n)$ may be obtained using the conditional variance formula
$$\V(X_{n}) = \E[\V(X_{n}|X_{n-1})] + \V(\E[X_{n}|X_{n-1}]).$$
Again from that $X_{n} =\Sum_{i=1}^{X_{n-1}}Z_{n,i}$, we have
$$
\E[X_{n}|X_{n-1}] = X_{n-1}\mu,\quad \V(X_{n}|X_{n-1}) = X_{n-1}\sigma^2
$$
and hence
\begin{align*}
\V(\E[X_{n}|X_{n-1}]) &= \V(X_{n-1}\mu)=\mu^2\V(X_{n-1})\\
\E[\V(X_{n}|X_{n-1})] &= \sigma^2\E[X_{n-1}]=\sigma^2\mu^{n-1}\E[X_0].
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Variance of a Branching Process}
So
\begin{align*}
\V(X_{n})&=\sigma^2\mu^{n-1}\E[X_0]+\mu^2\V(X_{n-1})\\
&=\sigma^2\mu^{n-1}\E[X_0]+\mu^2(\sigma^2\mu^{n-2}\E[X_0]+\mu^2\V(X_{n-2}))\\
&=\sigma^2(\mu^{n-1}+\mu^n)\E[X_0]+\mu^4\V(X_{n-2})\\
&=\sigma^2(\mu^{n-1}+\mu^n)\E[X_0]+\mu^4(\sigma^2\mu^{n-3}\E[X_0]+\mu^2\V(X_{n-3}))\\
&=\sigma^2(\mu^{n-1}+\mu^n+\mu^{n+1})\E[X_0]+\mu^6\V(X_{n-3})\\
&\qquad\vdots\\
&=\sigma^2(\mu^{n-1}+\mu^{n}+\ldots+\mu^{2n-2})\E[X_0]+\mu^{2n}\V(X_0)\\
&=\begin{cases}
\sigma^2\mu^{n-1}\left(\frac{1-\mu^{n}}{1-\mu}\right)\E[X_0] +\mu^{2n}\V(X_0)& \text{if } \mu\neq1\\
n\sigma^2\E[X_0] + \V(X_0)&\text{if } \mu=1
\end{cases}
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.1 The Gambler's Ruin Problem}
\begin{itemize}
\item A gambler repeatedly plays a game until he goes bankrupt or his fortune reaches $N$.
\item In each game, he can win \$1 with probability $p$ or lose \$1 with probability $q=1-p$.
\item Outcomes of different games are independent
\item Define $X_n=$ the gambler's fortune after the $n$th game.
\item $\{X_n\}$ is a simple random walk w/ absorbing boundaries at 0 and $N$.
\[P_{00} = P_{NN} = 1, \;P_{i,i+1} = p, P_{i,i-1}=q ,\; i = 1, 2,\ldots, N - 1\]
\item Two recurrent classes: $\{0\}$ and $\{N\}$\\
one transient class $\{1, 2,\ldots, N - 1\}$
\item Regardless of the initial fortune $X_0$, eventually $\lim_{n\to\infty}X_n=0$ or $N$
as all states are transient except 0 or $N$.
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.1 The Gambler's Ruin Problem}\small
Denote $A$ as the event that the gambler's fortune reaches $N$ before reaches 0.
Then
\[P_i=P(A|X_0=i).\]

Conditioning on the outcome of the first game,
\begin{align*}
P_i&=P(A|X_0=i, \alert{\text{he wins the 1st game}})
     \underbrace{P(\alert{\text{he wins the 1st game}})}_{\alert{=p}}\\
  &\qquad+ P(A|X_0=i,\alert{\text{he loses the 1st game}})
     \underbrace{P(\alert{\text{he loses the 1st game}})}_{\alert{=q}}\\
&=P(A|X_0=i,\structure{X_1=i\!+\!1})\alert{p}+ P(A|X_0=i,\structure{X_1=i\!-\!1})\alert{q}\\
&=\underbrace{P(A|X_1=i+1)}_{\alert{=P_{i+1}}}p+
\underbrace{P(A|X_1=i-1)}_{\alert{=P_{i-1}}}q \;(\because\text{ Markov})
\end{align*}
We get a set of equations
\begin{align*}
P_i&=pP_{i+1}+qP_{i-1} \quad\text{for }i=1,2,\ldots,N-1.\\
P_0&=0,\quad P_N=1
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Solving the equations $P_i=pP_{i+1}+qP_{i-1}$}
\begin{align*}
\alert<2->{(p+q)}P_i&=pP_{i+1}+qP_{i-1}&\alert<2->{\text{since }p+q=1}\\
\Leftrightarrow\quad q(P_i- P_{i-1})&=p(P_{i+1}- P_i)\\
\Leftrightarrow\quad P_{i+1}- P_i&=(q/p)(P_i- P_{i-1})
\end{align*}
As $P_0=0$,
\begin{align*}
P_2- P_1&=(q/p)(P_1- P_0)=(q/p)P_1\\
P_3- P_2&=(q/p)(P_2- P_1)=(q/p)^2P_1\\[-3pt]
&\vdots\\[-3pt]
P_i- P_{i-1}
&=(q/p)(P_{i-1}-P_{i-2})=(q/p)(q/p)^{i-2}P_1=(q/p)^{i-1}P_1
\end{align*}
Adding up the equations above we get
\[P_i-P_1=\left[q/p+(q/p)^2+\cdots+(q/p)^{i-1}\right]P_1\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
From
\[P_i-P_1=\left[q/p+(q/p)^2+\cdots+(q/p)^{i-1}\right]P_1\]
we get
\[
P_i=
\begin{cases}
\frac{1-(q/p)^i}{1-(q/p)}P_1 &\text{if }p\neq q\\
i P_1 &\text{if }p=q
\end{cases}
\]
As $P_N=1$, we get
\[
P_1=
\begin{cases}
\frac{1-(q/p)}{1-(q/p)^N} &\text{if }p\neq 0.5\\
1/N &\text{if }p=0.5
\end{cases}
\]
So
\[
P_i=
\begin{cases}
\frac{1-(q/p)^i}{1-(q/p)^N} &\text{if }p\neq 0.5\\
i/N &\text{if }p=0.5
\end{cases}
\]
If the gambler will never quit with whatever fortune he has ($N=\infty$),
then
\[
\lim_{N\to\infty}P_i=
\begin{cases}
1-(q/p)^i &\text{if }p>0.5\\
0 &\text{if }p\le 0.5
\end{cases}
\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.3 Random Walk w/ Reflective Boundary at 0}
\begin{itemize}
\item State Space $=\{0,1,2,\ldots\}$
\item $P_{01}=1, P_{i,i+1}=p, \;P_{i,i-1}=1-p=q,\;\text{for } i=1,2,3\ldots$
\item Only one class, irreducible
\item For $i<j$, define
\begin{align*}
N_{ij}&=\min\{m> 0: X_m=j|X_0=i\}\\
&=\text{first time to reach state $j$ when starting from state $i$}
\end{align*}
\item Observe that $N_{0n}=N_{01}+N_{12}+\ldots+N_{n-1,n}$\\
By the Markov property, $N_{01}$, $N_{12},\ldots,N_{n-1,n}$ are indep.
\item Given $X_0=i$
\begin{equation}\label{eq:T1TT}
N_{i,i+1}=
\begin{cases}
1 & \text{if } X_1=i+1\\
1 + N^{*}_{i-1,i}+ N^{*}_{i,i+1} &\text{if } X_1=i-1
\end{cases}
\end{equation}
Observe that $N^{*}_{i,i+1}\sim N_{i,i+1}$, and $N^{*}_{i,i+1}$ is indep of $N^{*}_{i-1,i}$.
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.3 Random Walk w/ Reflective Boundary at 0 (Cont'd)}
Let $m_i=\E(N_{i,i+1})$. Taking expected value on Equation \eqref{eq:T1TT}, we get
$$
m_i = \E[N_{i,i+1}]=
1+ q\E[N^{*}_{i-1,i}]+q\E[N^{*}_{i,i+1}]=1+q(m_{i-1}+m_i)
$$
Rearrange terms we get $p m_i=1+q m_{i-1}$ or
\begin{align*}
m_i
&=\frac{1}{p}+\frac{q}{p} m_{i-1}\\
&=\frac{1}{p}+\frac{q}{p}(\frac{1}{p}+\frac{q}{p}m_{i-2})\\
&=\frac{1}{p}\left[1+\frac{q}{p}+(\frac{q}{p})^2+\ldots+(\frac{q}{p})^{i-1}\right]+(\frac{q}{p})^im_{0}
\end{align*}
Since $N_{01}=1$, which implies $m_0=1$.
$$
m_i=
\begin{cases}
\frac{1-(q/p)^i}{p-q}+(\frac{q}{p})^i &\text{if } p\neq 0.5\\
2i+1 & \text{if } p=0.5
\end{cases}
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Mean of $N_{0,n}$}
Recall that $N_{0n}=N_{01}+N_{12}+\ldots+N_{n-1,n}$
\begin{align*}
\E[N_{0n}]&=m_{0}+m_{1}+\ldots+m_{n-1}\\
&=\begin{cases}
\frac{n}{p-q}-\frac{2pq}{(p-q)^2}[1-(\frac{q}{p})^n] &\text{if } p\neq 0.5\\
n^2 & \text{if } p= 0.5
\end{cases}
\end{align*}
When
$$
\begin{array}{cll}
p>0.5& \E[N_{0n}]\approx \frac{n}{p-q}-\frac{2pq}{(p-q)^2} & \text{linear in }n\\[5pt]
p=0.5& \E[N_{0n}]= n^2 & \text{quadratic in }n\\[5pt]
p<0.5& \E[N_{0n}]=O(\frac{2pq}{(p-q)^2}(\frac{q}{p})^n) & \text{exponential in }n
\end{array}
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284}
A Markov chain has transition probability matrix
\[
P=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0.2 & 0.4 &  0  & 0.3 &  0  & 0.1\cr
2 & 0.1 & 0.3 &  0  & 0.4 &  0  & 0.2\cr
3 &  0  &  0  & 0.3 & 0.7 &  0  &  0 \cr
4 &  0  &  0  & 0.6 & 0.4 &  0  &  0 \cr
5 &  0  &  0  &  0  &  0  & 0.5 & 0.5\cr
6 &  0  &  0  &  0  &  0  & 0.2 & 0.8
}
\]
Communicating classes:
\[
\begin{array}{ccc}
\{1,2\}&\{3,4\}&\{5,6\}\\
\uparrow &\uparrow &\uparrow \\
\text{transient}&\text{recurrent}&\text{recurrent}
\end{array}
\]
Find $\lim_{n\to\infty}P^{(n)}.$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
Observe that
$\lim_{n\to\infty}P^{(n)}_{ij}=0$ if $j$ is transient, hence,
\[
\lim_{n\to\infty}P^{(n)}=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & \alert{0} & \alert{0} &  ?  &  ?  &  ?  &  ? \cr
2 & \alert{0} & \alert{0} &  ?  &  ?  &  ?  &  ? \cr
3 & \alert{0} & \alert{0} &  ?  &  ?  &  ?  &  ? \cr
4 & \alert{0} & \alert{0} &  ?  &  ?  &  ?  &  ? \cr
5 & \alert{0} & \alert{0} &  ?  &  ?  &  ?  &  ? \cr
6 & \alert{0} & \alert{0} &  ?  &  ?  &  ?  &  ?
}
\]

\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
Observe that
$\lim_{n\to\infty}P^{(n)}_{ij}=0$ if $j$ is NOT accessible from $i$
\[
\lim_{n\to\infty}P^{(n)}=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0 & 0 &  ?  &  ?  &  ?  &  ? \cr
2 & 0 & 0 &  ?  &  ?  &  ?  &  ? \cr
3 & 0 & 0 &  ?  &  ?  & \alert{0} & \alert{0}\cr
4 & 0 & 0 &  ?  &  ?  & \alert{0} & \alert{0}\cr
5 & 0 & 0 & \alert{0} & \alert{0} &  ?  &  ? \cr
6 & 0 & 0 & \alert{0} & \alert{0} &  ?  &  ?
}
\]
The two classes \{3,4\} and \{5,6\} do not communicate
and hence the transition probabilities in between are all 0.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
Since the Markov chain restricted to the closed class \{3,4\} is also a Markov chain
with the transition matrix
$
\bordermatrix{%
  &  3  &  4  \cr
3 & 0.3 & 0.7 \cr
4 & 0.6 & 0.4
}
$ and the limiting distribution of a two-state Markov chain
with the transition matrix
$\left(
\begin{array}{cc}
1-\alpha  & \alpha\cr
\beta & 1-\beta
\end{array}\right)
$
is $\left(\dfrac{\beta}{\alpha+\beta},\dfrac{\alpha}{\alpha+\beta}\right),$
we get
\[
\lim_{n\to\infty}P^{(n)}=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0 & 0 &  ?  &  ?  &  ?  &  ? \cr
2 & 0 & 0 &  ?  &  ?  &  ?  &  ? \cr
3 & 0 & 0 & \alert{6/13} & \alert{7/13} & 0 & 0\cr
4 & 0 & 0 & \alert{6/13} & \alert{7/13} & 0 & 0\cr
5 & 0 & 0 & 0 & 0 &  ?  &  ? \cr
6 & 0 & 0 & 0 & 0 &  ?  &  ?
}
\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
\[
P=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0.2 & 0.4 &  0  & 0.3 &  0  & 0.1\cr
2 & 0.1 & 0.3 &  0  & 0.4 &  0  & 0.2\cr
3 &  0  &  0  & 0.3 & 0.7 &  0  &  0 \cr
4 &  0  &  0  & 0.6 & 0.4 &  0  &  0 \cr
5 &  0  &  0  &  0  &  0  & 0.5 & 0.5\cr
6 &  0  &  0  &  0  &  0  & 0.2 & 0.8
}
\]
For the same reason,
\[
\lim_{n\to\infty}P^{(n)}=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0 & 0 &  ?  &  ?  &  ?  &  ? \cr
2 & 0 & 0 &  ?  &  ?  &  ?  &  ? \cr
3 & 0 & 0 & \alert{6/13} & \alert{7/13} & 0 & 0\cr
4 & 0 & 0 & \alert{6/13} & \alert{7/13} & 0 & 0\cr
5 & 0 & 0 & 0 & 0 & \alert{2/7} & \alert{5/7} \cr
6 & 0 & 0 & 0 & 0 & \alert{2/7} & \alert{5/7}
}
\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
\small
\begin{minipage}{0.4\textwidth}
It remains to find $$\pi_{ij}=\lim_{n\to\infty}P^{(n)}_{ij}$$
from a transient state $i=1,2$ to a recurrent state $j=3$, 4, 5, or 6.%\par\medskip
\end{minipage}\quad
%Starting from state 1 or 2, the process is either absorbed to the class $\{3,4\}$ or
%the class $\{5,6\}.$ Define
%\begin{align*}
%\pi_{1,\{3,4\}}&=\lim_{n\to\infty} P(X_n=3\text{ or }4~|~X_0=1)\\
%\pi_{2,\{3,4\}}&=\lim_{n\to\infty} P(X_n=3\text{ or }4~|~X_0=2)
%\end{align*}
\begin{minipage}{0.55\textwidth}\footnotesize
\begin{flushright}
$P=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0.2 & 0.4 &  0  & 0.3 &  0  & 0.1\cr
2 & 0.1 & 0.3 &  0  & 0.4 &  0  & 0.2\cr
3 &  0  &  0  & 0.3 & 0.7 &  0  &  0 \cr
4 &  0  &  0  & 0.6 & 0.4 &  0  &  0 \cr
5 &  0  &  0  &  0  &  0  & 0.5 & 0.5\cr
6 &  0  &  0  &  0  &  0  & 0.2 & 0.8
}
$
\end{flushright}
\end{minipage}\par\medskip

By the Chapman-Kolmogorov Equation,
\begin{align*}
P^{(n+1)}_{13}&
=P_{11}P^{(n)}_{13}
+P_{12}P^{(n)}_{23}
+P_{13}P^{(n)}_{33}
+P_{14}P^{(n)}_{43}
+P_{15}P^{(n)}_{53}
+P_{16}P^{(n)}_{63}\\
&=0.2P^{(n)}_{13}+0.4P^{(n)}_{23}+0+0.3P^{(n)}_{43}+0+0.1\underbrace{P^{(n)}_{63}}_{=0}
\end{align*}
where $P^{(n)}_{63}=0$ since state 3 and 6 do not communicate.

Let $n\to \infty$ and recall we've shown earlier that $\lim_{n\to\infty}P^{(n)}_{43}=6/13$.
We get the equation
$$\pi_{13}=0.2\pi_{13}+0.4\pi_{23}+0.3\times\frac{6}{13}.$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
Similarly,
\begin{align*}
P^{(n+1)}_{23}&
=P_{21}P^{(n)}_{13}
+P_{22}P^{(n)}_{23}
+P_{23}P^{(n)}_{33}
+P_{24}P^{(n)}_{43}
+P_{25}P^{(n)}_{53}
+P_{26}P^{(n)}_{63}\\
&=0.1P^{(n)}_{13}+0.3P^{(n)}_{23}+0+0.4P^{(n)}_{43}+0+0.2\underbrace{P^{(n)}_{63}}_{=0}
\end{align*}
where $P^{(n)}_{63}=0$ since state 3 and 6 do not communicate.
Let $n\to \infty$ and recall we've shown earlier that $\lim_{n\to\infty}P^{(n)}_{43}=6/13$.
We get the equation
$$\pi_{23}=0.1\pi_{13}+0.3\pi_{23}+0.4\times\frac{6}{13}.$$
Along with the equation $\pi_{13}=0.2\pi_{13}+0.4\pi_{23}+0.3\times\frac{6}{13} $ obtained
on the previous page, we get
$$
\pi_{13}=\frac{37}{52}\times\frac{6}{13}=\frac{111}{338}, \quad\pi_{23}=\frac{35}{52}\times\frac{6}{13}=\frac{105}{338}
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
Similarly
\begin{align*}
P^{(n+1)}_{15}&
=P_{11}P^{(n)}_{15}
+P_{12}P^{(n)}_{25}
+P_{13}P^{(n)}_{35}
+P_{14}P^{(n)}_{45}
+P_{15}P^{(n)}_{55}
+P_{16}P^{(n)}_{65}\\
&=0.2P^{(n)}_{15}+0.4P^{(n)}_{25}+0+0.3\underbrace{P^{(n)}_{45}}_{=0}+0+0.1P^{(n)}_{65}\\
P^{(n+1)}_{25}&
=P_{21}P^{(n)}_{15}
+P_{22}P^{(n)}_{25}
+P_{23}P^{(n)}_{35}
+P_{24}P^{(n)}_{45}
+P_{25}P^{(n)}_{55}
+P_{26}P^{(n)}_{65}\\
&=0.1P^{(n)}_{15}+0.3P^{(n)}_{25}+0+0.4\underbrace{P^{(n)}_{45}}_{=0}+0+0.2P^{(n)}_{65}
\end{align*}
where $P^{(n)}_{45}=0$ since state 4 and 5 do not communicate.
Letting $n\to \infty$ and since $\lim_{n\to\infty}P^{(n)}_{65}=2/7$,
we get the equations
\begin{align*}
\pi_{15}&=0.2\pi_{15}+0.4\pi_{25}+0.1(2/7)\\
\pi_{25}&=0.1\pi_{15}+0.3\pi_{25}+0.2(2/7)
\end{align*}
and can find the solutions
$$
\pi_{15}=\frac{15}{52}\times\frac{2}{7}=\frac{15}{182}, \quad\pi_{25}=\frac{17}{52}\times\frac{2}{7}=\frac{17}{182}.
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Exercise 4.50 on p.284 (Cont'd)}
One can use the same method to find that
\begin{align*}
\pi_{14}&=\frac{37}{52}\times\frac{7}{13}, \quad\pi_{24}=\frac{35}{52}\times\frac{7}{13}\\
\pi_{16}&=\frac{15}{52}\times\frac{5}{7}, \quad\pi_{26}=\frac{17}{52}\times\frac{5}{7}
\end{align*}
Hence,
\[
\lim_{n\to\infty}P^{(n)}=
\bordermatrix{%
  &  1  &  2  &  3  &  4  &  5  &  6 \cr
1 & 0 & 0 &  \frac{37}{52}\times\frac{6}{13} & \frac{37}{52}\times\frac{7}{13}  &  \frac{15}{52}\times\frac{2}{7}  &  \frac{15}{52}\times\frac{5}{7} \cr
2 & 0 & 0 &  \frac{35}{52}\times\frac{6}{13} & \frac{35}{52}\times\frac{7}{13}  &  \frac{17}{52}\times\frac{2}{7}  &  \frac{17}{52}\times\frac{5}{7} \cr
3 & 0 & 0 & 6/13 & 7/13 & 0 & 0\cr
4 & 0 & 0 & 6/13 & 7/13 & 0 & 0\cr
5 & 0 & 0 & 0 & 0 & 2/7 & 5/7 \cr
6 & 0 & 0 & 0 & 0 & 2/7 & 5/7
}
\]

\end{frame}
% ----------------------------------------------------------------------
\end{document}
