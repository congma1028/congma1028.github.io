%\documentclass[letterpaper,draft]{beamer}
\documentclass[letterpaper,handout, mathserif]{beamer}
%\documentclass[letterpaper]{beamer}

%---multiple pages on one sheet, ADD for handout--
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=1mm]
%-------------------------------------------------
\usepackage{amsmath,amsfonts}
%\usepackage[misc]{ifsym} % for the dice symbol \Cube{}
%\usepackage{booktabs}
%\usepackage{mdwlist}
%\usepackage{pgf,tikz}
%\usetheme{Copenhagen}
%\usetheme{warsaw}
\setbeamertemplate{navigation symbols}{}
\usepackage[english]{babel}
\def\ul{\underline}
% or whatever

\usepackage[latin1]{inputenc}
\subject{Talks}

\def\Sum{\sum\nolimits}
\def\Prod{\prod\nolimits}
\def\p{\mathrm P}
\def\E{\mathbb E}
\def\V{\mathrm Var}
\def\typo#1{\alert{#1}}
%-------------Answers------------
\def\Hide#1#2{\ul{~~~\onslide<#1>{\alert{#2}}~~~}}
\def\hide#1#2{\ul{~~\onslide<#1>{\alert{#2}}~~}}
%------Centered Page Number------
\input{Centerpgn}
\def\chapnum{9}
%--------------------------------
\setbeamertemplate{footline}[centered page number]

\title{STAT253/317 Lecture \chapnum}
\date{}
\author{Cong Ma}
\begin{document}
% ----------------------------------------------------------------------
\begin{frame}\maketitle\bigskip\begin{center}Chapter 5 \ Poisson Processes\end{center}\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{5.2 Exponential Distribution}
Let $X$ follow exponential distribution with rate $\lambda$: $X\sim Exp(\lambda)$.
\begin{itemize}
\item Density: $f_X(x)=\lambda e^{-\lambda x}$ for $x\ge 0$
\item CDF: $F_X(x)=1- e^{-\lambda x}$ for $x\ge 0$
\item $\E(X)=1/\lambda$, $\V(X)=1/\lambda^2$
\item If $X_1,\ldots,X_n$ are i.i.d $Exp(\lambda)$, then $S_n=X_1+\cdots+X_n\sim Gamma(n,\lambda)$,
with density
$$f_{S_n}(x)=\lambda e^{-\lambda t}\frac{(\lambda t)^{n-1}}{(n-1)!}$$
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{The Exponential Distribution is Memoryless ($\star\star\star\star\star$)}
Lemma: for all $s, t \geq 0$
$$\p(X>t+s \mid X>t)=\p(X>s)$$
{\em Proof.}
\begin{align*}
\p(X>t+s|X>t)&=\frac{\p(X> t+s \mbox{ and }X>t)}{\p(X>t)}\\
&=\frac{\p(X> t+s)}{\p(X>t)}\\
&=\frac{e^{-\lambda(t+s)}}{e^{-\lambda t}}=e^{-\lambda s}=\p(X>s)
\end{align*}\bigskip

{\bf Implication}. If the lifetime of batteries has an Exponential distribution,
then {\em a used battery is as good as a new one}, as long as it's not dead!
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Another Important Property of the Exponential}
If $X_1,\ldots,X_n$ are independent, $X_i,\sim Exp(\lambda_i)$ for $i=1,\ldots,n$ then
\begin{itemize}
\item[(i)] $\min(X_1,\ldots,X_n)\sim Exp(\lambda_1+\cdots+\lambda_n)$, and
\item[(ii)] $\p\big(X_{j}=\min(X_1,\ldots,X_n)\big)=\displaystyle\frac{\lambda_j}{\lambda_1+\cdots+\lambda_n}$
\end{itemize}
{\em Proof of (i)}
\begin{align*}
&\p(\min(X_1,\ldots,X_n) > t) = \p(X_1 > t,\ldots,X_n > t)\\
={}& \p(X_1 > t)\ldots\p(X_n > t)= e^{-\lambda_1 t}\cdots e^{-\lambda_n t}\\
={}& e^{-(\lambda_1+\cdots+\lambda_n)t}.
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
{\em Proof of (ii)}
\begin{align*}
&\p\big(X_{j}=\min(X_1,\ldots,X_n)\big)\\
={}&\p(X_j < X_i\mbox{ for }i =1,\ldots,n, i\neq j)\\
={}&\int^{\infty}_0 \p(X_j < X_i\mbox{ for }i \neq j|X_j = t)\lambda_j e^{-\lambda_j t}dt\\
={}&\int^{\infty}_0 \p(t < X_i\mbox{ for }i \neq j)\lambda_j e^{-\lambda_j t}dt\\
={}&\int^{\infty}_0\lambda_j e^{-\lambda_j t} \prod_{i\neq j} \p(X_i > t)dt\\
={}&\int^{\infty}_0 \lambda_j e^{-\lambda_j t}\prod_{i\neq j} e^{-\lambda_i t}dt\\
={}& \lambda_j\int^{\infty}_0 e^{-(\lambda_1+\cdots+\lambda_n)t}dt\\
={}& \frac{\lambda_j}{\lambda_1+\cdots+\lambda_n}
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 5.8: Post Office}
\begin{itemize}
\item A post office has two clerks.
\item Service times for clerk $i$ $\sim Exp(\lambda_i)$, $i = 1$, 2
\item When you arrive, both clerks are busy but no one else waiting. You will enter service
when either clerk becomes free.
\item Find $\E[T]$, where $T=$ the amount of time you spend in the post office.
\end{itemize}
{\bf Solution.}
Let $R_i=$ remaining service time of the customer with clerk $i$, $i = 1$, 2.
\begin{itemize}
\item Note $R_i$'s are indep. $\sim Exp(\lambda_i)$, $i = 1$, 2 by the memoryless property
\item Observe $T = \min(R_1, R_2) + S$ where $S$ is your service time
\item Using the property of exponential distributions,
$$
\min(R_1, R_2)\sim Exp(\lambda_1+\lambda_2)\quad\Rightarrow\quad \E[\min(R_1, R_2)]=\frac{1}{\lambda_1+\lambda_2}
$$
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 5.8: Post Office (Cont'd)}
As for your service time $S$, observe that
$$
S\sim
\begin{cases}
Exp(\lambda_1) &\text{if }R_1 < R_2\\
Exp(\lambda_2) &\text{if }R_2 < R_1
\end{cases}
\Rightarrow
\begin{array}{l}
\E[S|R_1 < R_2] = 1/\lambda_1\\
\E[S|R_2 < R_1] = 1/\lambda_2
\end{array}
$$
Recall that $\p(R_1 < R_2)=\lambda_1/(\lambda_1+\lambda_2)$
So
\begin{align*}
\E[S]&= \E[S|R_1 < R_2]\p(R_1 < R_2)+\E[S|R_2 < R_1]\p(R_2 < R_1)\\
&=\frac{1}{\lambda_1}\times\frac{\lambda_1}{\lambda_1+\lambda_2}+
\frac{1}{\lambda_2}\times\frac{\lambda_2}{\lambda_1+\lambda_2}=\frac{2}{\lambda_1+\lambda_2}
\end{align*}
Hence the expected amount of time you spend in the post office is
\begin{align*}
\E[T] &= \E[\min(R_1, R_2)] + \E[S]\\
&= \frac{1}{\lambda_1+\lambda_2} + \frac{2}{\lambda_1+\lambda_2} =\frac{3}{\lambda_1+\lambda_2}.
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{5.3.1. Counting Processes}
A counting process $\{N(t)\}$ is a cumulative count of number of events happened up to time $t$.

\begin{block}{Definition.}
A stochastic processes $\{N(t),t\ge 0\}$ is a \structure{\em counting process} satisfying
\begin{itemize}
\item [(i)] $N(t)=0,1,\ldots$ (integer valued),
\item [(ii)] If $s <t$, then $N(s)\le N(t)$.
\item [(iii)] For $s < t$, $N(t) - N(s)=$ number of events that occur in the interval $(s, t]$.
\end{itemize}
\end{block}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
\begin{block}{Definition.}
A process $\{X(t), t\ge 0\}$ is said to have \structure{\em stationary increments} if for any $t>s$, the distribution of $X(t)-X(s)$ depends on $s$ and $t$ only through the
difference $t-s$, for all $s < t$.\par
That is, $X(t+a)-X(s+a)$ has the same distribution as $X(t)-X(s)$ for any constant $a$.
\end{block}\medskip
\begin{block}{Definition.}
A process $\{X(t), t\ge 0\}$ is said to have \structure{\em independent increments} if  for any $s_1< t_1\le s_2< t_2\le\ldots\le s_k<t_k$, the random variable $X(t_1)-X(s_1)$, $X(t_2)-X(s_2),\ldots,X(t_k)-X(s_k)$ are independent, i.e. the numbers of events that occur in {\bf disjoint} time intervals are {\bf independent}.
\end{block}\bigskip

{\bf Example}. Modified simple random walk $\{X_n, n\ge 0\}$ is a process with independent and stationary increment, since
$X_n=\sum_{k=0}^n \xi_k$ where $\xi_k$'s are i.i.d with $\p(\xi_k=1)=p$ and $\p(\xi_k=0)=1-p$.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Definition 5.1 of Poisson Processes}
A Poisson process with rate $\lambda>0$ $\{N(t), t\ge 0\}$ is a counting process satisfying
\begin{itemize}
\item [(i)] $N(0)=0$,
\item [(ii)] For $s<t$, $N(t)-N(s)$ is independent of $N(s)$ (independent increment)
\item [(iii)] For $s < t$, $N(t) - N(s)\sim Poi(\lambda(t-s))$, i.e.,
$$\p(N(t) - N(s)=k)=e^{-\lambda(t-s)}\frac{(\lambda(t-s))^k}{k!}$$
\end{itemize}
\bigskip

{\bf Remark}: In (iii), the distribution of $N(t) - N(s)$ depends on $t-s$ only, not $s$, which
implies $N(t)$ has stationary increment.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Definition 5.3 of Poisson Processes}
The counting process $\{N(t), t \ge 0\}$ is said to be a Poisson
process having rate $\lambda$, $\lambda  > 0$, if
\begin{itemize}
\item[(i)] $N(0) = 0$.
\item[(ii)] The process has stationary and independent increments.
\item[(iii)] $\p(N(h) = 1) = \lambda h+o(h)$.
\item[(iv)] $\p(N(h) \ge 2) = o(h)$.
\end{itemize}\bigskip

\structure{\bf Theorem 5.1}\; Definitions 5.1 and 5.3 are equivalent.

{\em [Proof of Definitions 5.1 $\Rightarrow$ Definition 5.3]}

From Definitions 5.1, $N(h)\sim Poi(h)$. Thus
\begin{align*}
\p(N(h) =1)&=\lambda h e^{-\lambda h} = \lambda h+o(h)\\
\p(N(h) \ge 2)&=1-\p(N(h)=0)-\p(N(h)=1)\\
&=1-e^{-\lambda h}-\lambda he^{-\lambda h}=o(h)
\end{align*}

{\em Proof of Definitions 5.3 $\Rightarrow$ Definition 5.1}:\par
See textbook.
\end{frame}
% ----------------------------------------------------------------------
\end{document}
