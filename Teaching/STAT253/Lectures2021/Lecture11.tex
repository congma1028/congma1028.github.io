%\documentclass[letterpaper,draft]{beamer}
\documentclass[letterpaper,handout, mathserif]{beamer}
%\documentclass[letterpaper]{beamer}

%---multiple pages on one sheet, ADD for handout--
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=1mm]
%-------------------------------------------------
\usepackage{amsmath,amsfonts}
%\usepackage[misc]{ifsym} % for the dice symbol \Cube{}
%\usepackage{booktabs}
%\usepackage{mdwlist}
%\usepackage{pgf,tikz}
%\usetheme{Copenhagen}
%\usetheme{warsaw}
\setbeamertemplate{navigation symbols}{}
\usepackage[english]{babel}
\def\ul{\underline}
% or whatever

\usepackage[latin1]{inputenc}
\subject{Talks}

\def\Sum{\sum\nolimits}
\def\Prod{\prod\nolimits}
\def\p{\mathrm P}
\def\E{\mathbb E}
\def\V{\mathrm{Var}}
\def\typo#1{\alert{#1}}
%-------------Answers------------
\def\Hide#1#2{\ul{~~~\onslide<#1>{\alert{#2}}~~~}}
\def\hide#1#2{\ul{~~\onslide<#1>{\alert{#2}}~~}}
\def\hid#1#2{\onslide<#1>{\alert{#2}}}
%------Centered Page Number------
\input{Centerpgn}
\def\chapnum{11}
%--------------------------------
\setbeamertemplate{footline}[centered page number]

\title{STAT253/317 Lecture \chapnum} \date{} \author{Cong Ma}
\begin{document}
% ----------------------------------------------------------------------
\begin{frame}\maketitle
\begin{center}
\begin{tabular}{ll}
5.3 & The Poisson Processes\\
5.4 & Generalizations of the Poisson Processes
\end{tabular}
\end{center}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Superposition}
The sum of two independent Poisson processes with respective rates $\lambda_1$ and $\lambda_2$, called the {\bf superposition} of the processes, is again a Poisson process but with rate $\lambda_1+\lambda_2$.\par\medskip

The proof is straight forward from Definition 5.3 and hence omitted.\bigskip

{\bf Remark}: By repeated application of the above arguments we can
see that the superposition of $k$ independent Poisson processes with
rates $\lambda_1,\cdots,\lambda_k$ is again a Poisson process with rate $\lambda_1+\cdots+\lambda_k$.
\end{frame}
% ----------------------------------------------------------------------
%\begin{frame}{Why Are Poisson Processes Commonly Used?}
%A useful result in probability theory:
%\begin{quote}
%if we take $N$ \underline{independent} counting processes and sum them up,
%then the resulting superposition process is approximately a Poisson process.
%\end{quote}
%Here
%\begin{itemize}
%\item $N$ must be ``large enough'' and
%\item the rates of the individual processes must be ``small'' relative to $N$
%\item but the individual processes that go into the superposition can otherwise be arbitrary.
%\end{itemize}
%
%E.g., $N(t)=\# $ of crimes by time $t$ in a certain town.
%\vspace{-12pt}\begin{flushright}
%\begin{tabular}{|c|c|c|c|c|c|c|}  \hline
%   &  &  &  &  &  &  \\\hline
%   &  &  &  &  &  &  \\\hline
%   &  &  &  &  &  &  \\\hline
%   &  &  &  &  &  &  \\\hline
%   &  &  &  &  &  &  \\\hline
%   &  &  &  &  &  &  \\\hline
%\end{tabular}
%\end{flushright}
%\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Thinning}
Consider a Poisson process $\{N(t): t\ge 0\}$ with rate $\lambda$.\par
At each arrival of events, it is classified as a
$$
\begin{cases}
\mbox{Type 1 event with probability }& p\qquad \mbox{ or}\\
\mbox{Type 2 event with probability }&1-p,
\end{cases}
$$
independently of all other events.
Let $$N_i(t)= \mbox{\# of type }i \mbox{ events occurred during }[0, t], \ i = 1,2.$$
Note that $N(t) = N_1(t) +N_2(t)$.\bigskip

\begin{block}{Proposition 5.2}
$\{N_1(t), t \ge0\}$ and $\{N_2(t), t\ge 0\}$ are both Poisson processes
having respective rates $\lambda p$ and $\lambda (1 - p)$.\par\smallskip
Furthermore, the two processes are \underline{independent}.
\end{block}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Proof of Proposition 5.2}
First observe that given $N(t)=n+m$,
$$N_1(t)\sim Binomial(n+m,p).\qquad\qquad\mbox{(why?)}$$
\begin{align*}
\mbox{Thus\quad }&\p(N_1(t)=n,N_2(t)=m)\\
={}&\p(N_1(t)=n,N_2(t)=m|N(t)=n+m)\p(N(t)=n+m)\\
={}&{n+m\choose n}p^n(1-p)^m e^{-\lambda t}\frac{(\lambda t)^{n+m}}{(n+m)!}\\
={}&e^{-\lambda tp}\frac{(\lambda pt)^{n}}{n!}e^{-\lambda t(1-p)}\frac{(\lambda (1-p)t)^{m}}{m!}\\
={}&\p(N_1(t)=n)\p(N_2(t)=m).
\end{align*}
This proves the independence of $N_1(t)$ and $N_2(t)$ and that
$$N_1(t)\sim Poisson(\lambda p t),\quad N_2(t)\sim Poisson(\lambda (1-p)t).$$
Both $\{N_1(t)\}$ and $\{N_2(t)\}$ inherit the stationary and independent increment properties  from $\{N(t)\}$, and hence are both Poisson processes.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Some ``Converse'' of Thinning \& Superposition}
Consider two indep. Poisson processes $\{N_A(t)\}$ and $\{N_B(t)\}$ w/ respective rates $\lambda_A$ and $\lambda_B$.
Let \begin{align*}
S^A_{n}&= \mbox{arrival time of the $n$th $A$ event} \\
S^B_{m}&= \mbox{arrival time of the $m$th $B$ event}
\end{align*}
Find $\p(S^A_{n} < S^B_{m})$.\bigskip

{\bf Approach 1}:

Observer that $S^A_{n}\sim Gamma(n,\lambda_A)$, $S^B_{m}\sim Gamma(m,\lambda_B)$ and they are independent.
Thus
$$
\p(S^A_{n} < S^B_{m})
=\int_{x<y}\lambda_Ae^{-\lambda_A x}\frac{(\lambda_A x)^{n-1}}{(n-1)!}  \lambda_Be^{-\lambda_B y}\frac{(\lambda_B y)^{m-1}}{(m-1)!}dxdy
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Some ``Converse'' of Thinning \& Superposition (Cont'd)}
Let $N(t)=N_A(t)+N_B(t)$ be the superposition of the two processes.
Let $$I_i=\begin{cases}1 &\mbox{if the $i$th event in the superpositon process is an $A$ event}\\ 0&\mbox{otherwise}\end{cases}.$$

\vspace{-10pt}
The $I_i$, $i=1,2,\ldots$ are i.i.d. Bernoulli$(p)$, where $p=\dfrac{\lambda_A}{\lambda_A+\lambda_B}$.\medskip

{\bf Approach 2}:
\begin{align*}
\p(S^A_{n} < S^B_{1})
&=\p(\mbox{the first $n$ events are all $A$ events})=\left(\frac{\lambda_A}{\lambda_A+\lambda_B}\right)^n
\end{align*}\vspace{-10pt}
\begin{align*}
&\p(S^A_{n} < S^B_{m})
=\p(\mbox{at least $n$ $A$ events occur before $m$ $B$ events})\\
&=\p(\mbox{at least $n$ heads before $m$ tails})\\
&=\p(\mbox{at least $n$ heads in the first $n+m-1$ tosses})\\
&=\sum_{k=n}^{n+m-1}{n+m-1\choose k}\left(\frac{\lambda_A}{\lambda_A+\lambda_B}\right)^k\left(\frac{\lambda_B}{\lambda_A+\lambda_B}\right)^{n+m-1-k}
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Proposition 5.3 (Generalization of Proposition 5.2)}
Consider a Poisson process with rate $\lambda$.
If an event occurs at time $t$ will be classified as a type $i$ event with probability $p_i(t)$, $i=1,\ldots,k$,
$\sum_i p_i(t) =1$, for all $t$, independently of all other events.
then
$$N_i(t)= \mbox{number of type }i \mbox{ events occurring in }[0, t], \ i = 1,\ldots,k.$$
Note $N(t)=\sum_{i=1}^{k}N_i(t)$. Then
$N_i(t)$, $i=1,\ldots,k$ are independent Poisson random variables with means $\lambda\int_0^t p_i(s)ps.$\bigskip


Remark: Note $\{N_i(t), t\ge 0\}$ are NOT Poisson processes.
%\item[(b)] $\{N_i(t),t\ge 0\}$, $i=1,\ldots,k$ are $k$ independent nonhomogeneous Poisson processes with respective intensity function $\lambda p_i(s)$.
%\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example}% (Revision of Exercise 6.66 on p.364)
\begin{itemize}
\item Policyholders of a certain insurance company have accidents occurring
according to a Poisson process with rate $\lambda$.
\item The amount of time $T$ from when the
accident occurs until a claim is made has distribution $G(t)=\p(T\le t)$.
\item Let $N_c(t)$ be the number of claims made by time $t$.
\end{itemize}
Find the distribution of $N_c(t)$.\bigskip

{\em Solution.} Suppose an accident occurred at time $s$.
It is claimed by time $t$ if $s+T\le t$, i.e., with probability $$p(s)=\p(T\le t-s)=G(t-s).$$
We call an accident type I if it's completed before $t$, and type II otherwise.
By Proposition 5.3, $N_c(t)$ has a Poisson distribution with mean
$$
\lambda\int_0^t p(s)ps=\lambda\int_0^t G(t-s)ds=\lambda\int_0^t G(s)ds
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{5.4.1 Nonhomogeneous Poisson Process}\noindent
{\bf Definition 5.4a.}
A nonhomogeneous (a.k.a. non-stationary) Poisson process with intensity function $\lambda(t)\ge 0$  is a counting process $\{N(t), t\ge 0\}$ satisfying
\begin{itemize}
\item[(i)] $N(0) = 0$.
\item[(ii)] having independent increments.
\item[(iii)] $\p(N(t+h)-N(t) = 1) = \lambda(t) h+o(h)$.
\item[(iv)] $\p(N(t+h)-N(t)\ge 2) = o(h)$.
\end{itemize}\bigskip


{\bf Definition 5.4b.}
A nonhomogeneous Poisson process with intensity function $\lambda(t)\ge 0$  is a counting process $\{N(t), t\ge 0\}$ satisfying
\begin{itemize}
\item [(i)] $N(0)=0$,
\item [(ii)] for $s,t\ge 0$, $N(t+s)-N(s)$ is independent of $N(s)$ (independent increment)
\item [(iii)] For $s,t\ge 0$, $N(t+s) - N(s)\sim Poisson(m(t+s)-m(s))$, where $m(t)=\int_0^t\lambda(u)du$
\end{itemize}\medskip

The two definitions are equivalent.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{The Interarrival Times of a Nonhomogeneous Poisson Process Are NOT Independent}
A nonhomogeneous Poisson process {\bf has independent increment} but
its {\bf interarrival times} between events are
\begin{itemize}
\item neither independent
\item nor identically distributed.
\end{itemize}
{\it Proof}. Homework.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Proposition 5.4}
Let $\{N_1(t), t\ge 0\}$, and $\{N_2(t), t\ge 0\}$ be two independent nonhomogeneous Poisson process with respective intensity functions $\lambda_1(t)$ and $\lambda_2(t)$, and let $N(t)=N_1(t)+N_2(t)$. Then\medskip
\begin{itemize}
\item[(a)] $\{N(t), t\ge 0\}$ is a nonhomogeneous Poisson process with intensity function
$\lambda_1(t)+\lambda_2(t)$.
\item[(b)] Given that an event of the $\{N(t), t\ge 0\}$ process occurs at time $t$ then, independent
of what occurred prior to $t$ , the event at $t$ was from the $\{N_1(t)\}$ process with probability
$$\frac{\lambda_1(t)}{\lambda_1(t)+\lambda_2(t)}.$$
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{5.4.2 Compound Poisson Processes}
\mbox{}{\bf Definition.} Let $\{N(t)\}$ be a (homogeneous) Poisson process with rate $\lambda$ and $Y_1$, $Y_2,\ldots$ are i.i.d random variables independent of $\{N(t)\}$. The process
$$X(t)=\Sum_{i=1}^{N(t)}Y_i$$
is called a {\em compound Poisson process,} in which $X(t)$ is defined as 0 if $N(t)=0$.

\medskip\hrule\medskip

A compound Poisson process has
\begin{itemize}
\item {\bf independent increment}, since $X(t+s)-X(s)=\sum_{i=1}^{N(t+s)-N(s)}Y_{i+N(s)}$ is independent of $X(s)=\Sum_{i=1}^{N(s)}Y_i$, and
\item {\bf stationary increment}, since $X(t+s)-X(s)=\sum_{i=1}^{N(t+s)-N(s)}Y_{i+N(s)}$ has the same distribution as
$X(t)=\Sum_{i=1}^{N(t)}Y_i$
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{The Mean of a Compound Poisson Process}
Suppose $\E[Y_i]=\mu_Y$, $\V(Y_i)=\sigma^2_Y$. Note that $\E[N(t)]=\lambda t$.
\begin{align*}
\E[X(t)|N(t)]&=\Sum_{i=1}^{N(t)}\E[Y_i|N(t)]\\
&=\Sum_{i=1}^{N(t)}\E[Y_i]\quad (\mbox{since } Y_i\mbox{'s are indep. of }N(t))\\
&=N(t)\mu_Y
\end{align*}
Thus
$$
\E[X(t)]=\E[\E[X(t)|N(t)]]=\E[N(t)]\mu_Y=\lambda t\mu_Y
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Variance of a Compound Poisson Process (Cont'd)}
Similarly, using that $\E[N(t)]=\V(N(t))=\lambda t$, we have
\begin{align*}
\V[X(t)|N(t)]&=\V\left(\Sum_{i=1}^{N(t)}Y_i\Big|N(t)\right)\\
&=\Sum_{i=1}^{N(t)}\V(Y_i|N(t))\\
&=\Sum_{i=1}^{N(t)}\V(Y_i)\quad (\mbox{since } Y_i\mbox{'s are indep. of }N(t))\\
&=N(t)\sigma^2_Y\\
\E[\V(X(t)|N(t))]&=\E[N(t)\sigma^2_Y]=\lambda t\sigma^2_Y\\
\V(\E[X(t)|N(t)])&=\V(N(t)\mu_Y)=\V(N(t))\mu^2_Y=\lambda t\mu^2_Y
\end{align*}
Thus
\begin{align*}
\V(X(t))&=\E[\V[X(t)|N(t)]]+\V(\E[X(t)|N(t)])\\
&=\lambda t(\sigma^2_Y+\mu^2_Y)=\lambda t\E[Y_i^2]
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{CLT of a Compound Poisson Process}
As $t\to\infty$, the distribution of
$$
\frac{X(t)-\E[X(t)]}{\sqrt{\V(X(t))}}
=\frac{X(t)-\lambda t\mu_Y}{\sqrt{\lambda t(\sigma^2_Y+\mu^2_Y)}}$$
converges to a standard normal distribution $N(0,1)$.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{5.4.3 Conditional Poisson Processes}
\mbox{}{\bf Definition.}
A {\em conditional} (or {\em mixed}) {\em Poisson process} $\{N(t), t\ge 0\}$ is a counting process satisfying
\begin{itemize}
\item [(i)] $N(0)=0$,
\item [(ii)] having stationary increment, and
\item [(iii)] there is a random variable $\Lambda>0$ with probability density $g(\lambda)$, such that
given $\Lambda=\lambda$,
$$N(t+s) - N(s)\sim Poisson(\lambda t),$$
i.e.,
\end{itemize}
$$\p(N(t+s)-N(s)=k)=\int_0^{\infty}e^{-\lambda t}\frac{(\lambda t)^k}{k!}g(\lambda)d\lambda, \;k=0,1,\ldots$$

%\medskip\hrule\medskip


\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
\noindent
\textbf{Remark:} In general, a conditional Poisson process does NOT have independent increment.
\begin{align*}
%\p(N(s)=j, N(t+s)-N(s)=k|\Lambda=\lambda)&=e^{-\lambda s}\frac{(\lambda s)^j}{j!}e^{-\lambda t}\frac{(\lambda t)^k}{k!}\\
&\p(N(s)=j, N(t+s)-N(s)=k)\\
&=\int_0^{\infty}e^{-\lambda s}\frac{(\lambda s)^j}{j!}e^{-\lambda t}\frac{(\lambda t)^k}{k!}g(\lambda)d\lambda\\
&\neq\left(\int_0^{\infty}e^{-\lambda s}\frac{(\lambda s)^j}{j!}g(\lambda)d\lambda\right)
  \left(\int_0^{\infty}e^{-\lambda t}\frac{(\lambda t)^k}{k!}g(\lambda)d\lambda\right)\\
&= \p(N(s)=j)\p(N(t+s)-N(s)=k)\\
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\end{document} 