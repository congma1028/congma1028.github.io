%\documentclass[letterpaper,draft]{beamer}
\documentclass[letterpaper,handout]{beamer}
%\documentclass[letterpaper]{beamer}

%---multiple pages on one sheet, ADD for handout--
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=1mm]
%-------------------------------------------------
\usepackage{amsmath,amsfonts}
%\usepackage{booktabs}
%\usepackage{mdwlist}
\usepackage{amsfonts}
%\usetheme{Copenhagen}
%\usetheme{warsaw}
\setbeamertemplate{navigation symbols}{}
\usepackage[english]{babel}
\def\ul{\underline}
% or whatever

\usepackage[latin1]{inputenc}
\subject{Talks}

\def\Sum{\sum\nolimits}
\def\Prod{\prod\nolimits}
\def\P{\mathbb{P}}
\def\p{\mathrm P}
\def\E{\mathbb E}
\def\V{\mathrm{Var}}
\def\CV{\mathrm{Cov}}
\def\X{\mathcal{X}}
\def\dt{\Delta}
\def\typo#1{\alert{#1}}
%-------------Answers------------
\def\Hide#1#2{\ul{~~~\onslide<#1>{\alert{#2}}~~~}}
\def\hide#1#2{\ul{~~\onslide<#1>{\alert{#2}}~~}}
%------Centered Page Number------
\input{Centerpgn}
\def\chapnum{21}
%--------------------------------
\setbeamertemplate{footline}[centered page number]

\title{STAT253/317 Winter 2022 Lecture \chapnum} \date{} \author{Cong Ma}
\begin{document}
% ----------------------------------------------------------------------
\begin{frame}\maketitle
\bigskip

\begin{center}\large
\begin{tabular}{ll}
Section 8.7 & The Model G/M/1\\
%Chapter 10 & Brownian Motion
\end{tabular}
\end{center}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{8.7 The Model G/M/1}
The $G/M/1$ model assumes
\begin{itemize}
\item i.i.d times between successive arrivals with an arbitrary distribution $G$
\item i.i.d service times $\sim \mbox{Exp}(\mu)$
\item a single server; and
\item first come, first serve
\end{itemize}
Just like $M/G/1$ system, there is also a discrete-time Markov chain embedded in an $G/M/1$ system. Let
\begin{align*}
Y_n &=\#\mbox{ of customers in the system seen by the $n$th arrival},\; n\ge 1\\
D_n &=\#\mbox{ of customers the server can possibly serve}\\
&\quad\mbox{  between the $(n-1)$st and the $n$th arrival},\; n\ge 1
\end{align*}
Observed that $\{Y_n, n\ge 0\}$ and $\{D_n,n\ge 1\}$ are related as follows
$$
Y_{n+1} = %(Y_n+1-D_{n+1})^+=
\begin{cases}
Y_n+1-D_{n+1} & \mbox{if } Y_n+1\ge D_{n+1}\\
0 & \mbox{if } Y_n+1 < D_{n+1}
\end{cases},\quad n\ge 1
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{A Markov Chain embedded in $G/M/1$ (Cont'd)}
\begin{itemize}
\item By the memoryless property of the exponential service time,
the remaining service time of the customer being served at an arrival is also $\sim\mbox{Exp}(\mu).$

\item Thus starting from the $(n-1)$st arrival, the events of completion of servicing a customer constitute a Poisson process of rate $\mu$.
\item Let $G_n$ be the time elapsed between the $(n-1)$st and the $n$th arrival.
\item Then given $G_n$, $D_n$ is Poisson with mean $\mu G_n$.
\item As $G_n$'s are i.i.d $\sim G$, we can conclude that $D_1,D_2,\ldots$ are i.i.d. with distribution
\begin{align*}
\delta_k = \p(D_n=k)&=\int_0^{\infty} \p(D_n=k|G_n=y)G(dy)\\
&=\int_0^{\infty} \frac{(\mu y)^k}{k!}e^{-\mu y}G(dy)
\end{align*}
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{A Markov Chain embedded in $G/M/1$ (Cont'd)}
The transition probabilities $P_{ij}$ for the Markov chain $\{Y_n, n\ge 0\}$ are thus:
\begin{align*}
P_{ij} &= \p(Y_{n+1}=j|Y_n=i)\\
&=
\begin{cases}
\p(D_{n+1}\ge i+1)=\sum_{k=i+1}^{\infty}\delta_k  & \mbox{if }j=0\\
\p(D_{n+1}=i+1-j)=\delta_{i+1-j}, & \mbox{if }j\ge 1, i+1 \ge j\\
0 &\mbox{if }i+1<j
\end{cases}
\end{align*}
i.e., the transition probability matrix is
$$\P=
\bordermatrix{%
   &  0  &  1  &  2  &  3  &  4  &\cdots\cr
0  & \sum_{k=1}^{\infty}\delta_k & \delta_0 &  0  &  0  &  0  &\cdots \cr
1  & \sum_{k=2}^{\infty}\delta_k & \delta_1 & \delta_0 &  0  &  0  &\cdots \cr
2  & \sum_{k=3}^{\infty}\delta_k & \delta_2 & \delta_1 & \delta_0 &  0  &\cdots \cr
3  & \sum_{k=4}^{\infty}\delta_k & \delta_3 & \delta_2 & \delta_1 & \delta_0 &\cdots \cr
%4  & \sum_{k=5}^{\infty}\delta_k & \delta_4 & \delta_3 & \delta_2 & \delta_1 &\cdots \cr
\vdots&\vdots&\vdots&\vdots&\vdots&\vdots& \ddots
}
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{A Markov Chain embedded in $G/M/1$ (Cont'd)}
To find the stationary distribution $\pi_i=\lim_{n\to\infty}\p(Y_n=i)$, $i=0,1,2,\ldots$,
we have to solve the equations
$$
\pi_j = \sum_{i=0}^{\infty}\pi_i P_{ij}=\sum_{i=j-1}^{\infty}\pi_i \delta_{i+1-j},\; j\ge 1\quad
\mbox{and}\quad\sum_{j=0}^{\infty}\pi_j = 1
$$
Let us try a solution of the form $\pi_j=c\beta^j$, $j\ge 0$.
Substituting into the equation above leads to
\begin{align*}
c\beta^j &=\Sum_{i=j-1}^{\infty}c\beta^i \delta_{i+1-j}\quad (\mbox{Divide both sides by }c\beta^{j-1})\\
\Rightarrow\quad
\beta &= \Sum_{i=j-1}^{\infty}\beta^{i+1-j} \delta_{i+1-j} = \Sum_{i=0}^{\infty}\beta^{i}\delta_{i}
\end{align*}
Observe that $\Sum_{i=0}^{\infty}\beta^{i}\delta_{i}$ is exactly the generating function of $D_n$
$g(s)=\E[s^{D_n}]$ taking value at $s=\beta.$\par
Thus if we can find $0<\beta<1$ such that $\beta = g(\beta)$, then
$$\pi_j = (1-\beta)\beta^j,\quad j\ge 0$$ is a stationary distribution of $\{Y_n\}.$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{A Markov Chain embedded in $G/M/1$ (Cont'd)}
\textbf{Claim: } \\

The equation $$\beta = g(\beta)$$ has a solution between 0 and 1 iff $g'(1)=E[D_n]=\mu\E[G_n]>1$.
%\vspace{1.5in}

\vfill
This condition is intuitive since if
\begin{align*}
&\mbox{the average service time }1/\mu \\
&>\;\mbox{the average interarrival time of customers }\E[G_n],
\end{align*}
the queue will become longer and longer and the system will ultimately explode.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{PASTA Principle Does Not Apply to G/M/1}
With the stationary distribution $\{\pi_j,\, j\ge 0\}$,
one might attempt to calculate $L$, the average number of customers in the system as
$$\E[Y_n]=\Sum_{k=0}^{\infty}k \pi_k = \Sum_{k=0}^{\infty} k(1-\beta)\beta^k =\frac{\beta}{1-\beta}.$$
However, the PASTA principle does not apply as the arrival process is not Poisson.
Recall
\begin{align*}
a_k=\pi_k&=\mbox{proportion of arrivals see $k$ in the system}\\
  P_k&=\mbox{proportion of time having $k$ customers in the system},
\end{align*}
%Observe that $\pi_k\neq P_k$ since the longer the interarrival time $G_n$, the larger $D_n$, and hence the smaller $Y_{n}$.
%Hence $$L=\Sum_{k=0}^{\infty}kP_k\neq \E[Y_n]=\Sum_{k=0}^{\infty}k\pi_k.$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{$W$ of G/M/1}
Though we cannot use $\{\pi_j\}$ to find $L$, we can use it to find $W$.
Let $W_n$ be the waiting time of $n$th customer in the system.
If he/she sees $k$ customers at arrival, then $W_n$ is the total service time of $k+1$ customers. That is, \begin{align*}
\E[W_n|Y_n=k]&=\E[\mbox{sum of $k+1$ i.i.d. Exp}(\mu)\mbox{ service times}]\\
&=\frac{k+1}{\mu}.
\end{align*}
Thus
\begin{align*}
W &= \sum_{k=0}^{\infty}\E[W_n|Y_n=k]\p(Y_n=k)=\sum_{k=0}^{\infty}\E[W_n|Y_n=k]\pi_k\\
&=\sum_{k=0}^{\infty}\frac{k+1}{\mu}(1-\beta)\beta^k = \frac{1}{\mu(1-\beta)}
\end{align*}
Here we use the identity $\displaystyle\sum_{k=0}^{\infty}(k+1)x^k= \dfrac{1}{(1 - x)^2}.$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{$L$, $W_Q$, $L_Q$ of G/M/1}
By the Little's Formula, we know $L=\lambda W$, in which $\lambda$ is the arrival rate of customers, which is the reciprocal of the mean interarrival time $\E[G_n]$
$$\lambda = \frac{1}{\E[G_n]}$$
Thus
$$L=\lambda W = \frac{1}{\E[G_n]}\frac{1}{\mu(1-\beta)}=\frac{1}{\mu\E[G_n](1-\beta)}$$
Moreover,
\begin{align*}
W_Q&=W-\E[\mbox{Service Time}] = W-\frac{1}{\mu}=\frac{\beta}{\mu(1-\beta)}\\
L_Q&=\lambda W_Q = \frac{\beta}{\mu\E[G_n](1-\beta)}
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{8.9.3 $G/M/k$}
Just like $G/M/1$ system, $G/M/k$ system can also be analyzed as a Markov Chain. Let
\begin{align*}
Y_n &=\#\mbox{ of customers in the system seen by the $n$th arrival},\; n\ge 1\\
D_n &=\#\mbox{ of customers the $k$ servers can possibly serve}\\
&\quad\mbox{  between the $(n-1)$st and the $n$th arrival},\; n\ge 1
\end{align*}
Observed again that $\{Y_n, n\ge 0\}$ and $\{D_n,n\ge 1\}$ are related as follows
$$
Y_{n+1} = %(Y_n+1-D_{n+1})^+=
\begin{cases}
Y_n+1-D_{n+1} & \mbox{if } Y_n+1\ge D_{n+1}\\
0 & \mbox{if } Y_n+1 < D_{n+1}
\end{cases},\quad n\ge 1
$$
One can show that the distribution of $D_{n+1}$ depends on $Y_n$ but not $Y_{n-1},Y_{n-2},\ldots$
and hence $\{Y_{n}\}$ is a Markov chain. The transition probabilities are given in p.544-545 (p.565-566 in 10ed)
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{8.9.4 $M/G/k$}
Unlike $G/M/k$, the method to analyze $M/G/1$ cannot be used to analyze $M/G/k$.
If we follow the lines as we do in $M/G/1$
\begin{align*}
Y_n &=\#\mbox{ of customers in the system}\\
&\quad\mbox{ leaving behind at the $n$th departure},\; n\ge 1\\
D_n &=\#\mbox{ of customers entered the system}\\
&\quad\mbox{ during the service time of the $n$th customer},\; n\ge 1
\end{align*}
As there are more than one server, the service times are not disjoint,
and hence $D_n$'s are not independent.\bigskip

In fact, there is NO known exact formula for $L$, $W$, $L_Q$, $W_Q$ of an $M/G/k$ system.
\end{frame}
% ----------------------------------------------------------------------
\end{document}
\begin{frame}{10.1 Brownian Motion as a Limit of Random Walk}
The Browian motion arises as a limit of rescaled sum of i.i.d. random variables.
Let $X_1,X_2,\ldots$ be i.i.d random variables, $\E[X_i]=0$, $\V(X_i)=\sigma^2$. Define
$$
X(t)=\dt x(X_1+\ldots+X_{\lfloor t/\dt t\rfloor})
$$
where $\lfloor t/\dt t\rfloor$ is the integer part of $t/\dt t$.
Since
$$
\E[X(t)]=0,\quad
\V(X(t))=\sigma^2(\dt x)^2\left\lfloor \frac{t}{\dt t} \right\rfloor,
$$
to have a non-trivial limit, we would need to maintain that $\dt t=c(\dt x)^2$ as $\dt t$ and $\dt x\to 0$.
Let's take $c=1.$ In this case, as $\dt t\to 0$, $\dt x\to 0$, and $\dt t=(\dt x)^2$
$$
\E[X(t)]=0,\quad\V(X(t))\to\sigma^2t,
$$
Moreover, since $\dt x=\sqrt{\dt t}$, by CLT
$$
X(t)=\dt x(X_1+\ldots+X_{\lfloor \frac{t}{\dt t} \rfloor})\approx\sqrt{t}\sigma\frac{X_1+\ldots+X_{\lfloor \frac{t}{\dt t}\rfloor}}{\sqrt{\lfloor t/\dt t\rfloor}\sigma}\to N(0,\sigma^2t)
$$
in distribution.
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
Observe that the discrete-time process $$\{X(t),\;t=n\dt t,\;n=0,1,2\ldots\}$$ has
\structure{\em independent} and \structure{\em stationary increments} since
\begin{align*}
X(s)&= \dt x(X_1+\ldots+X_{\lfloor \frac{s}{\dt t}\rfloor}),\;\mbox{and }\\[-2pt]
X(t)-X(s)&= \dt x(X_{\lfloor \frac{s}{\dt t}\rfloor+1}+\ldots+X_{\lfloor \frac{t}{\dt t}\rfloor})
\end{align*}
are independent, and for $t=l\dt t > s=m\dt t$, the distribution of $X(t)-X(s)$
depends on the number of terms $\lfloor \frac{t}{\dt t}\rfloor-\lfloor \frac{s}{\dt t}\rfloor$ $=(l-m)=(t-s)/(\dt t)$ in the sum, but not $s$.\smallskip

Thus the limit of $X(t)$ is a process with the following properties.\bigskip
\end{frame}
% ----------------------------------------------------------------------
%\begin{frame}%{Joint Distribution of $B(t_1),B(t_2), \ldots , B(t_n)$}
%To derive the joint density of $B(t_1),B(t_2), \ldots , B(t_n),$ %at $B(t_1)=x_1,B(t_2)=x_2,\ldots,B(t_n)=x_n$,
%we may first derive the joint density of
%\begin{align*}
%Y_1&=B(t_1)\sim N(0,\sigma^2t_1),\\
%Y_k&=B(t_k)-B(t_{k-1})\sim N(0,\sigma^2(t_k-t_{k-1}),\ k=2,\ldots,n.
%\end{align*}
%Since $B(t)$ has independent increments, $Y_1,\ldots,Y_n$ are independent. Their joint density is thus
%$$f_{Y_1}(y_1)f_{Y_2}(y_2)\ldots f_{Y_n}(y_n)
%=\prod_{k=1}^n\frac{\exp\left(-\frac{y_k^2}{2(t_k-t_{k-1})}\right)}{\sqrt{2\pi(t_k-t_{k-1})}}$$
%where $t_0=0$. Make a change of variables $B(t_1)=Y_1$, $B(t_k)=Y_1+\ldots+Y_k$, $k=2\ldots,n.$
%Note that the Jacobian is
%$$\det\left(\frac{\partial(y_1,y_2,\ldots,y_n)}{\partial(x_1,x_2,\ldots,x_n)}\right)=1$$
%The joint density of $B(t_1)=x_1,\ldots, B(t_n)=x_n$ is simply
%$$f_{Y_1}(x_1)f_{Y_2}(x_2-x_1)\ldots f_{Y_n}(x_n-x_{n-1})
%=\prod_{k=1}^n\frac{\exp\left(-\frac{(x_k-x_{k-1})^2}{2(t_k-t_{k-1})}\right)}{\sqrt{2\pi(t_k-t_{k-1})}}$$
%\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Conditional Distribution}
Given $B(t)=x$, what is the conditional distribution of $B(s)$?\bigskip

If $s>t$, since $B(s)-B(t)$ is independent of $B(t)$, the conditional distribution of $B(s)-B(t)$ given $B(t)$ is the same as the unconditional distribution of $B(s)-B(t)$, i.e., $N(0,\sigma^2(s-t)).$
Thus given $B(t)=x,$
\begin{align*}
B(s)&=B(t)+B(s)-B(t)\\
&=x+B(s)-B(t)\sim N(x,\sigma^2(s-t)).
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Conditional Distribution (Cont'd)}
What if $s<t$?\bigskip

Let's try to find a scalar $c$ such that $\CV(B(s)-cB(t),B(t))=0$
\begin{align*}
\CV(B(s)-cB(t),B(t))&=\CV(B(s),B(t))-\CV(B(t),B(t))\\
&=\sigma^2s-c\sigma^2t=\sigma^2(s-ct)
\end{align*}
Letting $c=s/t$, we have $\CV(B(s)-\frac{s}{t}B(t),B(t))=0$, which implies $B(s)-\frac{s}{t}B(t)$ and $B(t)$ are independent since they are jointly normal. Moreover, the conditional distribution of of $B(s)-\frac{s}{t}B(t)$ given $B(t)$ is the same as the unconditional distribution of $B(s)-\frac{s}{t}B(t)$, i.e., $N(0,\sigma^2\frac{s(t-s)}{t}).$
Thus given $B(t)=x,$
\begin{align*}
B(s)&=\frac{s}{t}B(t)+B(s)-\frac{s}{t}B(t)\\
&=\frac{sx}{t}+B(s)-\frac{s}{t}B(t)\sim N\left(\frac{sx}{t},\sigma^2\frac{s(t-s)}{t}\right).
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
\end{frame}
% ----------------------------------------------------------------------
