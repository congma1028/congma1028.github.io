%\documentclass[letterpaper,draft]{beamer}
\documentclass[letterpaper,handout, mathserif]{beamer}
%\documentclass[letterpaper]{beamer}

%---multiple pages on one sheet, ADD for handout--
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[letterpaper, landscape, border shrink=1mm]
%-------------------------------------------------
\usepackage{amsmath,amsfonts}
%\usepackage[misc]{ifsym} % for the dice symbol \Cube{}
%\usepackage{booktabs}
%\usepackage{mdwlist}
%\usepackage{pgf,tikz}
%\usetheme{Copenhagen}
%\usetheme{warsaw}
\setbeamertemplate{navigation symbols}{}
\usepackage[english]{babel}
\def\ul{\underline}
% or whatever

\usepackage[latin1]{inputenc}
\subject{Talks}

\def\Sum{\sum\nolimits}
\def\p{\mathrm P}
\def\E{\mathbb E}
\def\V{\mathrm{Var}}
%-------------Answers------------
\def\Hide#1#2{\ul{~~~\onslide<#1>{\alert{#2}}~~~}}
\def\hide#1#2{\ul{~~\onslide<#1>{\alert{#2}}~~}}
%------Centered Page Number------
\input{Centerpgn}
\def\chapnum{7}
%--------------------------------
\setbeamertemplate{footline}[centered page number]

\title{STAT253/317 Lecture \chapnum}
\date{}
\author{Cong Ma}
\begin{document}
% ----------------------------------------------------------------------
\begin{frame}\maketitle
\begin{center}
\begin{tabular}{ll}
$\bullet$ & Using the Recursive Relationship\\
4.5.3 & Random Walk w/ Reflective Boundary at 0\\
4.7   & Branching Processes
\end{tabular}
\end{center}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Utilize Recursive Relations of Markov Chains}

Law of total expectation/variance

\medskip
%Consecutive terms in many Markov chains $\{X_n\}$ often have some recursive relations like
%$$
%X_{n+1}= g(X_n, \xi_{n+1})\quad\text{for all }n
%$$
%where $\{\xi_n, n=0,1,2,\ldots\}$ are some i.i.d. random variables and $X_n$ is independent of $\{\xi_k: k>n\}$.
%\par\medskip

In many cases, we can use recursive relation to find $\E[X_n]$ and $\V[X_n]$ without knowing the exact distribution of $X_n$.
\begin{align*}
\E[X_{n+1}]&=\E[\E[X_{n+1}|X_{n}]]\\
\V(X_{n+1})&= \E[\V(X_{n+1}|X_{n})] + \V(\E[X_{n+1}|X_{n}])
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 1: Simple Random Walk}
$$
X_{n+1}=
\begin{cases}
X_n + 1 & \text{with prob }p\\
X_n - 1 & \text{with prob }q=1-p
\end{cases}
$$
So
\begin{align*}
\E[X_{n+1}|X_n] &= p (X_n+1) + q(X_n-1) = X_n + p-q\\
\V[X_{n+1}|X_n] &= 4pq
\end{align*}
Then
\begin{align*}
\E[X_{n+1}]&=\E[\E[X_{n+1}|X_n]] = \E[X_n] + p-q\\
\V(X_{n+1})&= \E[\V(X_{n+1}|X_{n})] + \V(\E[X_{n+1}|X_{n}])\\
&=\E[4pq]+\V(X_n + p-q) = 4pq + \V(X_n)
\end{align*}
So
$$
\E[X_n]=n(p-q)+\E[X_0],\qquad \V(X_n) = 4npq + \V(X_0)
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 2: Ehrenfest Urn Model with $M$ Balls}
Recall that
$$
X_{n+1}=
\begin{cases}
X_n+1 & \mbox{with probability } \frac{M-X_n}{M}\\
X_n-1 & \mbox{with probability } \frac{X_n}{M}
\end{cases}
$$
We have
$$
\E[X_{n+1}|X_n]=(X_n+1)\times \frac{M-X_n}{M}+(X_n-1)\times\frac{X_n}{M}=1+\left(1-\frac{2}{M}\right)X_n.
$$
Thus
$$
\E[X_{n+1}]=\E[\E[X_{n+1}|X_n]]=1+\left(1-\frac{2}{M}\right)\E[X_n]
$$
Subtracting $M/2$ from both sided of the equation above, we get
$$
\E[X_{n+1}]-\frac{M}{2}=\left(1-\frac{2}{M}\right)(\E[X_n]-\frac{M}{2})
$$
Thus
\begin{align*}
\E[X_n]-\frac{M}{2}
%&=\left(1-\frac{2}{M}\right)(\E[X_{n-1}]-\frac{M}{2})\\
%&=\left(1-\frac{2}{M}\right)\left(1-\frac{2}{M}\right)(\E[X_{n-2}]-\frac{M}{2})=\left(1-\frac{2}{M}\right)^2(\E[X_{n-2}]-\frac{M}{2})\\
%&=\left(1-\frac{2}{M}\right)^2\left(1-\frac{2}{M}\right)(\E[X_{n-3}]-\frac{M}{2})=\left(1-\frac{2}{M}\right)^3\E[X_{n-3}]-\frac{M}{2})\\
%&=\vdots\\
&=\left(1-\frac{2}{M}\right)^n(\E[X_{0}]-\frac{M}{2})
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Example 3: Branching Processes (Section 4.7)}
Consider a population of individuals.
\begin{itemize}
\item All individuals have the same lifetime
\item Each individual will produce a random number of offsprings at the end of its life
\end{itemize}
Let $X_n=$ size of the $n$-th generation, $n=0,1,2,\ldots$.

If $X_{n-1}=k$, the $k$ individuals in the $(n-1)$-th generation will independently produce $Z_{n,1}$, $Z_{n,2}$, \ldots, $Z_{n,k}$ new offsprings, and $Z_{n,1}$, $Z_{n,2}$, \ldots, $Z_{n,X_{n-1}}$ are i.i.d such that
$$P(Z_{n,i}=j)=P_j,\; j \ge 0.$$
We suppose that $P_j < 1$ for all $j \ge 0$.
\begin{equation}\label{eq:branching}
X_{n} =\Sum_{i=1}^{X_{n-1}}Z_{n,i}
\end{equation}
$\{X_n\}$ is a Markov chain with state space $=\{0,1,2,\ldots\}$ .
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Mean of a Branching Process}
Let $\mu=\E[Z_{n,i}]=\Sum_{j=0}^{\infty}jP_j$.
Since $X_{n} =\Sum_{i=1}^{X_{n-1}}Z_{n,i}$, we have
$$
\E[X_{n}|X_{n-1}]=\E\left[\Sum_{i=1}^{X_{n-1}}Z_{n,i}\Big|X_{n-1}\right]=X_{n-1}\E[Z_{n,i}]=X_{n-1}\mu
$$
So
$$
\E[X_{n}]=\E[\E[X_{n}|X_{n-1}]]=\E[X_{n-1}\mu]=\mu\E[X_{n-1}]
$$
If $X_0=1$, then
$$
\E[X_{n}]=\mu\E[X_{n-1}]=\mu^2\E[X_{n-2}]=\ldots=\mu^n\E[X_{0}]
$$
\begin{itemize}
\item If $\mu<1 \Rightarrow \E[X_{n}]\to 0$ as $n\to\infty \Rightarrow \lim_{n\to\infty}\p(X_n\ge1)=0$
the branching processes will eventually die out.
\item What if $\mu=1$ or $\mu>1$?
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Variance of a Branching Process}
Let $\sigma^2=\V[Z_{n,i}]=\Sum_{j=0}^{\infty}(j-\mu)^2P_j$.
$\V(X_n)$ may be obtained using the conditional variance formula
$$\V(X_{n}) = \E[\V(X_{n}|X_{n-1})] + \V(\E[X_{n}|X_{n-1}]).$$
Again from that $X_{n} =\Sum_{i=1}^{X_{n-1}}Z_{n,i}$, we have
$$
\E[X_{n}|X_{n-1}] = X_{n-1}\mu,\quad \V(X_{n}|X_{n-1}) = X_{n-1}\sigma^2
$$
and hence
\begin{align*}
\V(\E[X_{n}|X_{n-1}]) &= \V(X_{n-1}\mu)=\mu^2\V(X_{n-1})\\
\E[\V(X_{n}|X_{n-1})] &= \sigma^2\E[X_{n-1}]=\sigma^2\mu^{n-1}\E[X_0].
\end{align*}
So
\begin{align*}
\V(X_{n})&=\sigma^2\mu^{n-1}\E[X_0]+\mu^2\V(X_{n-1})\\
&=\sigma^2\E[X_0](\mu^{n-1}+\mu^{n}+\ldots+\mu^{2n-2})+\mu^{2n}\V(X_0)\\
&=\begin{cases}
\sigma^2\mu^{n-1}\left(\frac{1-\mu^{n}}{1-\mu}\right)\E[X_0] +\mu^{2n}\V(X_0)& \mbox{if } \mu\neq1\\
n\sigma^2\E[X_0] + \mu^{2n}\V(X_0)&\mbox{if } \mu=1
\end{cases}
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.1 The Gambler's Ruin Problem}
\begin{itemize}
\item A gambler repeatedly plays a game until he goes bankrupt or his fortune reaches $N$.
\item In each game, he can win \$1 with probability $p$ or lose \$1 with probability $q=1-p$.
\item Outcomes of different games are independent
\item Define $X_n=$ the gambler's fortune after the $n$th game.
\item $\{X_n\}$ is a simple random walk w/ absorbing boundaries at 0 and $N$.
\[P_{00} = P_{NN} = 1, \;P_{i,i+1} = p, P_{i,i-1}=q ,\; i = 1, 2,\ldots, N - 1\]
\item Two recurrent classes: $\{0\}$ and $\{N\}$\\
one transient class $\{1, 2,\ldots, N - 1\}$
\item Regardless of the initial fortune $X_0$, eventually $\lim_{n\to\infty}X_n=0$ or $N$
as all states are transient except 0 or $N$.
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.1 The Gambler's Ruin Problem}\small
Denote $A$ as the event that the gambler's fortune reaches $N$ before reaching 0.
Then
\[P_i=P(A|X_0=i).\]

Conditioning on the outcome of the first game,
\begin{align*}
P_i&=P(A|X_0=i, \alert{\text{he wins the 1st game}})
     \underbrace{P(\alert{\text{he wins the 1st game}})}_{\alert{=p}}\\
  &\qquad+ P(A|X_0=i,\alert{\text{he loses the 1st game}})
     \underbrace{P(\alert{\text{he loses the 1st game}})}_{\alert{=q}}\\
&=P(A|X_0=i,\structure{X_1=i\!+\!1})\alert{p}+ P(A|X_0=i,\structure{X_1=i\!-\!1})\alert{q}\\
&=\underbrace{P(A|X_1=i+1)}_{\alert{=P_{i+1}}}p+
\underbrace{P(A|X_1=i-1)}_{\alert{=P_{i-1}}}q \;(\because\text{ Markov})
\end{align*}
We get a set of equations
\begin{align*}
P_i&=pP_{i+1}+qP_{i-1} \quad\text{for }i=1,2,\ldots,N-1.\\
P_0&=0,\quad P_N=1
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Solving the equations $P_i=pP_{i+1}+qP_{i-1}$}
\begin{align*}
\alert<2->{(p+q)}P_i&=pP_{i+1}+qP_{i-1}&\alert<2->{\text{since }p+q=1}\\
\Leftrightarrow\quad q(P_i- P_{i-1})&=p(P_{i+1}- P_i)\\
\Leftrightarrow\quad P_{i+1}- P_i&=(1/p)(P_i- P_{i-1})
\end{align*}
As $P_0=0$,
\begin{align*}
P_2- P_1&=(q/p)(P_1- P_0)=(q/p)P_1\\
P_3- P_2&=(q/p)(P_2- P_1)=(q/p)^2P_1\\[-3pt]
&\vdots\\[-3pt]
P_i- P_{i-1}
&=(q/p)(P_{i-1}-P_{i-2})=(q/p)(q/p)^{i-2}P_1=(q/p)^{i-1}P_1
\end{align*}
Adding up the equations above we get
\[P_i-P_1=\left[q/p+(q/p)^2+\cdots+(q/p)^{i-1}\right]P_1\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}
From
\[P_i-P_1=\left[q/p+(q/p)^2+\cdots+(q/p)^{i-1}\right]P_1\]
we get
\[
P_i=
\begin{cases}
\frac{1-(q/p)^i}{1-(q/p)}P_1 &\text{if }p\neq q\\
i P_1 &\text{if }p=q
\end{cases}
\]
As $P_N=1$, we get
\[
P_N=
\begin{cases}
\frac{1-(q/p)}{1-(q/p)^N} &\text{if }p\neq 0.5\\
1/N &\text{if }p=0.5
\end{cases}
\]
So
\[
P_i=
\begin{cases}
\frac{1-(q/p)^i}{1-(q/p)^N} &\text{if }p\neq 0.5\\
i/N &\text{if }p=0.5
\end{cases}
\]
If the gambler will never quit with whatever fortune he has ($N=\infty$),
then
\[
\lim_{N\to\infty}P_i=
\begin{cases}
1-(q/p)^i &\text{if }p>0.5\\
0 &\text{if }p\le 0.5
\end{cases}
\]
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.3 Random Walk w/ Reflective Boundary at 0}
\begin{itemize}
\item State Space $=\{0,1,2,\ldots\}$
\item $P_{01}=1, P_{i,i+1}=p, \;P_{i,i-1}=1-p=q,\;\mbox{for } i=1,2,3\ldots$
\item Only one class, irreducible
\item For $i<j$, define
\begin{align*}
N_{ij}&=\min\{m> 0: X_m=j|X_0=i\}\\
&=\mbox{first time to reach state $j$ when starting from state $i$}
\end{align*}
\item Observe that $N_{0n}=N_{01}+N_{12}+\ldots+N_{n-1,n}$\\
By the Markov property, $N_{01}$, $N_{12},\ldots,N_{n-1,n}$ are indep.
\item Given $X_0=i$
\begin{equation}\label{eq:T1TT}
N_{i,i+1}=
\begin{cases}
1 & \mbox{if } X_1=i+1\\
1 + N^{*}_{i-1,i}+ N^{*}_{i,i+1} &\mbox{if } X_1=i-1
\end{cases}
\end{equation}
Observe that $N^{*}_{i,i+1}\sim N_{i,i+1}$, and $N^{*}_{i,i+1}$ is indep of $N^{*}_{i-1,i}$.
\end{itemize}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{4.5.3 Random Walk w/ Reflective Boundary at 0 (Cont'd)}
Let $m_i=\E(N_{i,i+1})$. Taking expected value on Equation \eqref{eq:T1TT}, we get
$$
m_i = \E[N_{i,i+1}]=
1+ q\E[N^{*}_{i-1,i}]+q\E[N^{*}_{i,i+1}]=1+q(m_{i-1}+m_i)
$$
Rearrange terms we get $p m_i=1+q m_{i-1}$ or
\begin{align*}
m_i
&=\frac{1}{p}+\frac{q}{p} m_{i-1}\\
&=\frac{1}{p}+\frac{q}{p}(\frac{1}{p}+\frac{q}{p}m_{i-2})\\
&=\frac{1}{p}\left[1+\frac{q}{p}+(\frac{q}{p})^2+\ldots+(\frac{q}{p})^{i-1}\right]+(\frac{q}{p})^im_{0}
\end{align*}
Since $N_{01}=1$, which implies $m_0=1$.
$$
m_i=
\begin{cases}
\frac{1-(q/p)^i}{p-q}+(\frac{q}{p})^i &\mbox{if } p\neq 0.5\\
2i+1 & \mbox{if } p=0.5
\end{cases}
$$
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}{Mean of $N_{0,n}$}
Recall that $N_{0n}=N_{01}+N_{12}+\ldots+N_{n-1,n}$
\begin{align*}
\E[N_{0n}]&=m_{0}+m_{1}+\ldots+m_{n-1}\\
&=\begin{cases}
\frac{n}{p-q}-\frac{2pq}{(p-q)^2}[1-(\frac{q}{p})^n] &\mbox{if } p\neq 0.5\\
n^2 & \mbox{if } p= 0.5
\end{cases}
\end{align*}
When
$$
\begin{array}{cll}
p>0.5& \E[N_{0n}]\approx \frac{n}{p-q}-\frac{2pq}{(p-q)^2} & \mbox{linear in }n\\[5pt]
p=0.5& \E[N_{0n}]= n^2 & \mbox{quadratic in }n\\[5pt]
p<0.5& \E[N_{0n}]=O(\frac{2pq}{(p-q)^2}(\frac{q}{p})^n) & \mbox{exponential in }n
\end{array}
$$
\end{frame}
% ----------------------------------------------------------------------
\end{document}
\begin{frame}{Extinction Probability}
Let $\pi_0$ denote the probability that the population will eventually die out, given $X_0 = 1$, i.e.
$$\pi_0 = \lim_{n\to\infty}\p(X_n = 0|X_0 = 1)$$

{\bf Fact 1.} The extinction probability $\pi_0$ is a root of of the equation
\begin{equation}\label{eq:extinct}
g(s)=s
\end{equation}
where $g(s)=\sum_{k=0}^{\infty}P_ks^k$ is the generating function of $Z_{n,i}$.

{\em Proof.}
\begin{align*}
\pi_0 &= \p(\mbox{population dies out})\\
&=\Sum_{j=0}^{\infty}\p(\mbox{population dies out}|X_{1}=j)P_j\\
&=\Sum_{j=0}^{\infty}\pi_0^jP_j=g(\pi_0)
\end{align*}
\end{frame}
% ----------------------------------------------------------------------
\begin{frame}

{\bf Fact 2.} Unless the $P_1=1$, when $\mu=\Sum_{j=0}^{\infty}jP_j\le 1$, the only root of \eqref{eq:extinct} within [0, 1] is 1.\smallskip

{\em Proof.}

Let $h(s)=g(s)-s$. Since $g(1)=1$, $g'(1)=\mu$, $h(1)=g(1)-1=0$,
$$h'(s)=\left(\sum_{j=1}^{\infty}jP_js^{j-1}\right)-1 < \left(\sum_{j=1}^{\infty}jP_j\right)-1=\mu-1 \quad\mbox{for }0\le s < 1$$
Thus if $\mu\le 1$, $h(s)$ is non-increasing in [0,1) since $h'(s) < 0$ for $0\le s< 1$.
So $h(s)> h(1)=0$ for $0\le s < 1$.\\ That is, $g(s)>s$ for $0\le s < 1$.

\bigskip\bigskip\hrule\medskip
{\bf Fact 3.} When $\mu>1$, the equation $g(s)=s$ has a unique root in [0,1), and that is $\pi_0$.
\end{frame}
% ----------------------------------------------------------------------
